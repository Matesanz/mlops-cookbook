{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\uddea The MLOps Cookbook","text":"<p>Welcome to the MLOps and MLFlow cookbook! In this cookbook, you will learn how to implement best practices of Machine Learning Operations (MLOps) using the open-source platform MLflow. MLOps is a set of practices and tools aimed at improving the collaboration and productivity of Machine Learning (ML) teams, as well as the reliability and reproducibility of ML models in production.</p> <p>What will I find in this cookbook?</p> <ul> <li>How to launch a MLflow server</li> <li>How to create and manage MLflow projects</li> <li>How to train and log ML models using MLflow</li> <li>How to deploy ML models as an API</li> </ul> <p>Diclaimer</p> <p>This cookbook is a work in progress and it's mainly focused on the use of MLFlow. If you find any errors or have suggestions for improvement, please let me know by creating an issue on the GitHub repository.</p> <p>Through this cookbook, you will learn how to use MLflow to manage the entire ML lifecycle, from data preparation and model training to deployment and monitoring. You will also learn how to create and manage MLflow projects, models, and registry, as well as how to deploy them as an API.</p> <p>Whether you are a data scientist, machine learning engineer, or software developer, this cookbook will provide you with the knowledge and skills to implement MLOps using MLFlow best practices and leverage the power of MLflow to streamline your ML workflow and improve the quality and reliability of your ML models.</p>"},{"location":"#what-is-mlops","title":"What is MLOps?","text":"<p>MLOps is like a set of tools and practices that help data scientists and machine learning engineers manage and deploy their machine learning models effectively. It's a bit like having a well-organized kitchen with all the tools and processes in place to turn a great recipe into a delicious meal.</p> <p></p> <p>In the world of machine learning, MLOps helps streamline the entire process, from developing and testing models to deploying them in real-world applications. It ensures that models are reliable, easy to update, and can be used by others in the organization. Just as a well-organized kitchen makes cooking easier and more efficient, MLOps makes working with machine learning models more efficient and reliable.</p> <p>MLOps simple analogy</p> <p>MLOps is a bit like having a well-organized kitchen with all the tools and processes in place to turn a great recipe into a delicious meal.</p>"},{"location":"about/","title":"About the Author","text":"<p>\ud83d\ude4b Hello! My name is Andr\u00e9s Matesanz, I'm a ML Engineer at Sngular. I built this cookbook to help my students at CodeSpace Academy to learn about MLOps at the data science bootcamp. You can find more about me at my Website: https://matesanz.github.io/</p>"},{"location":"setup/","title":"Setting Up the Environment","text":"<p>Setup everything in a few minutes</p> <p>Thanks to the magic of Docker and VSCode we can launch a running environment really quick! \ud83c\udf89</p> <p>You can choose how to do it:</p> <ul> <li>\ud83d\udc49 Setting the environment in my own computer</li> <li>\ud83d\udc49 Setting the environment using Github Codespaces</li> </ul>"},{"location":"setup/#local-environment","title":"Local Environment","text":"<p>Thanks to the magic of the DevContainers we can simply, clone the repo, open VSCode and launch a full ready environment clicking a single button. \ud83d\ude04</p> <p>You need Docker Installed</p> <p>In order to use DevContainers you need to have Docker installed in your computer. If you don't have it, you can download it from here.</p> <p>Install the VSCode pluggin \"Dev-Containers\"</p> <p>In order to use DevContainers you need to have the VSCode pluggin Dev Containers.</p> <p>You can do it manually:</p> <p></p> <p>Or by pressing:</p> <ol> <li>Press <code>Ctrl+P</code></li> <li>Paste <code>ext install ms-vscode-remote.remote-containers</code></li> <li>Press <code>Enter</code></li> </ol>"},{"location":"setup/#1-clone-the-repository","title":"1. Clone the repository","text":"<p>Run this command</p> <pre><code>git clone `https://github.com/&lt;your_user&gt;/mlops-course.git`\n</code></pre>"},{"location":"setup/#2-open-vscode","title":"2. Open VSCode","text":"<p>Launch VSCode and open the folder of the repo.</p> <p>On linux or Mac: Run this command</p> <pre><code>cd mlops-course/\ncode .\n</code></pre>"},{"location":"setup/#3-launch-the-devcontainer","title":"3. Launch the Devcontainer","text":"<p>Click on the bottom-right blue button <code>Reopen in Container</code> (wait, first time takes some time to run).</p> <p></p>"},{"location":"setup/#4-open-mlflow","title":"4. Open MLFlow","text":"<p>A MLFlow server should start automatically (thanks to the magic of Docker Compose) at http://localhost:5000/. If it did not started automatically (or if you launched VSCode on the web), then follow this instructions:</p> <ul> <li>\ud83d\udc49 Click on: <code>Ports</code></li> <li>\ud83d\udc49 Check there is a port 5000 open (if not add a port 5000)</li> <li>\ud83d\udc49 Click on: <code>\ud83c\udf10</code></li> </ul> <p></p> <p>Congrats!</p> <p>\ud83c\udf89 You are ready to go! \ud83c\udf89</p>"},{"location":"setup/#github-codespaces","title":"Github Codespaces","text":"<p>GitHub Codespaces is like having a ready-to-use computer for writing and testing code, but it's all online. Imagine if your coding tools, like your text editor and programming languages, were available on the internet. With GitHub Codespaces, you can write and run your code in a web browser without needing to install everything on your personal computer. It's handy for collaborating with others and quickly starting coding projects, kind of like having your coding workspace in the cloud.</p> <p></p> <p>You need a Github Account</p> <p>In order to access to GitHub Codespaces you need to log in into your account or register if you have no account. Github provides 60 hours of computing at CodeSpaces for free! \ud83e\udd11</p>"},{"location":"setup/#1-create-a-copy-of-the-repository-fork","title":"1. Create a copy of the repository (fork)","text":"<ul> <li>\ud83d\udc49 Go to the oficial repo at https://github.com/Matesanz/mlops-course</li> <li>\ud83d\udc49 Click on <code>Fork &gt; Create a new Fork</code></li> </ul>"},{"location":"setup/#2-launch-a-codespace-in-vscode","title":"2. Launch a CodeSpace in VSCode","text":"<ul> <li>\ud83d\udc49 Go to your fork at <code>https://github.com/&lt;your_user&gt;/mlops-course</code></li> <li>\ud83d\udc49 Click on: <code>&lt;&gt; Code</code> &gt; <code>Open in</code> &gt; <code>Open in Visual Studio Code</code></li> </ul> Accept all the pop-ups that appear Alternative: you can also launch VSCode in the web <p>\ud83d\udca1 If you have not installed VSCode at your computer this is a (even more) straightforward way to go.</p> <ul> <li>\ud83d\udc49 Go to your fork at <code>https://github.com/&lt;your_user&gt;/mlops-course</code></li> <li>\ud83d\udc49 Click on <code>Code &gt; Create codespace on main</code></li> </ul> <p></p> <ul> <li>\ud83d\udc49 Wait for the CodeSpace to launch</li> </ul> <p></p> <p>Don't forget to stop your CodeSpace once you've finished!</p> <p>Remember that you \"only\" have 60 hours / month of free CodeSpaces. But no worries if you forget to do it, after a time of inactivity (if you closed the tab), the CodeSpace will automatically turn off. \ud83d\udc4d</p> <p></p>"},{"location":"setup/#3-open-mlflow","title":"3. Open MLFlow","text":"<p>A MLFlow server should start automatically (thanks to the magic of Docker Compose) at http://localhost:5000/. If it did not started automatically (or if you launched VSCode on the web), then follow this instructions:</p> <ul> <li>\ud83d\udc49 Click on: <code>Ports</code></li> <li>\ud83d\udc49 Check there is a port 5000 open (if not add a port 5000)</li> <li>\ud83d\udc49 Click on: <code>\ud83c\udf10</code></li> </ul> <p></p> <p>Congrats!</p> <p>\ud83c\udf89 You are ready to go! \ud83c\udf89</p>"},{"location":"exercises/","title":"\u270d\ufe0f MLOps Exercises","text":"<p>In this section you will find the exercises of the MLOps cookbook. Exercises are Jupyter Notebooks that you can download and run in your local environment.</p>"},{"location":"exercises/#exercises","title":"\ud83d\udcda Exercises","text":"<ul> <li>Intro to MLFlow - Part I</li> <li>Intro to MLFlow - Part II</li> <li>Intro to MLFlow - Part III</li> <li>Intro to MLFlow - Part IV</li> <li>MLFlow for Kaggle - Part I</li> </ul>"},{"location":"exercises/1-intro_to_mlflow_I/","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part I","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied!"},{"location":"exercises/1-intro_to_mlflow_I/#exercise-intro-to-mlflow-part-i","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part I\u00b6","text":"<p>In this exercise, we will cover the basics of MLFlow. MLFlow is an open-source platform for the complete machine learning lifecycle. It is designed to work with any machine learning library and to be agnostic to the execution environment. It is also designed to be scalable and to support the complete machine learning lifecycle, including experimentation, reproducibility, and deployment.</p> <p>In this first part, we will cover the following topics:</p> <ul> <li>How to Install MLFlow.</li> <li>How to launch the MLFlow Server.</li> <li>How to create a new MLFlow Experiment.</li> <li>How to create a new MLFlow Run.</li> <li>How to log parameters, metrics, and artifacts.</li> </ul>"},{"location":"exercises/1-intro_to_mlflow_I/#how-to-install-mlflow","title":"How to Install MLFlow\u00b6","text":"<p>\ud83d\udca1 Remember: We can simply install MLFlow using pip \ud83c\udf89</p> <pre>pip install mlflow\n</pre>"},{"location":"exercises/1-intro_to_mlflow_I/#how-to-launch-the-mlflow-server","title":"How to launch the MLFlow Server\u00b6","text":"<p>\ud83d\udca1 Remember: After installing MLFlow, we can launch the MLFlow server using the following command in the terminal:</p> <pre>mlflow server\n</pre> <p>You will see the following output:</p> <pre>[2024-02-21 23:29:52 +0100] [725738] [INFO] Starting gunicorn 21.2.0\n[2024-02-21 23:29:52 +0100] [725738] [INFO] Listening at: http://127.0.0.1:5000 (725738)\n[2024-02-21 23:29:52 +0100] [725738] [INFO] Using worker: sync\n[2024-02-21 23:29:52 +0100] [725739] [INFO] Booting worker with pid: 725739\n[2024-02-21 23:29:52 +0100] [725740] [INFO] Booting worker with pid: 725740\n[2024-02-21 23:29:53 +0100] [725741] [INFO] Booting worker with pid: 725741\n</pre> <p>\ud83d\udc49 Then, we can access the mlflow server by opening the following URL in a web browser: http://localhost:5000.</p>"},{"location":"exercises/1-intro_to_mlflow_I/#exercise-i-connecting-to-the-mlflow-server","title":"Exercise I: Connecting to the MLFlow Server\u00b6","text":"<ol> <li>\ud83d\udc49 Connect to MLFlow using <code>mlflow.set_tracking_uri()</code> and set the URI to <code>http://localhost:5000</code>.</li> <li>\ud83d\udc49 Use <code>mlflow.search_experiments()</code> to list all the experiments.</li> </ol>"},{"location":"exercises/1-intro_to_mlflow_I/#exercise-ii-creating-a-new-mlflow-experiment","title":"Exercise II: Creating a New MLFlow Experiment\u00b6","text":"<ol> <li>\ud83d\udc49 Create a new MLFlow Experiment using <code>mlflow.create_experiment()</code> and set the name to <code>intro-to-mlflow</code>.</li> <li>\ud83d\udc49 Check if the experiment was created by using <code>mlflow.get_experiment_by_name()</code>.</li> <li>\ud83d\udc49 Print the experiment ID.</li> </ol>"},{"location":"exercises/1-intro_to_mlflow_I/#exercise-iii-creating-a-new-mlflow-run","title":"Exercise III: Creating a New MLFlow Run\u00b6","text":"<ol> <li>\ud83d\udc49 Create a new MLFlow Run using <code>mlflow.start_run()</code> and set the experiment_id to the ID of the <code>intro-to-mlflow</code> experiment.</li> <li>\ud83d\udc49 Check if the run was created by using <code>run.info.run_id</code>.</li> <li>\ud83d\udc49 Print the run_id.</li> </ol>"},{"location":"exercises/1-intro_to_mlflow_I/#exercise-iv-logging-tags-parameters-and-metrics","title":"Exercise IV: Logging Tags, Parameters and Metrics\u00b6","text":"<p>Imagine you have the following information about the run:</p> <ul> <li>model_type: \"RandomForest\"</li> <li>accuracy: 0.85</li> <li>max_depth: 10</li> <li>precision: 0.90</li> <li>learning_rate: 0.01</li> <li>recall: 0.80</li> </ul> <ol> <li>\ud83d\udc49 Think. What should you log as a tag, parameter, and metric?</li> <li>\ud83d\udc49 Create a new MLFlow Run using <code>mlflow.start_run()</code> and set the experiment_id to the ID of the <code>intro-to-mlflow</code> experiment.</li> <li>\ud83d\udc49 Log the tags using <code>mlflow.set_tags()</code>.</li> <li>\ud83d\udc49 Log the parameters using <code>mlflow.log_param()</code>.</li> <li>\ud83d\udc49 Log the metrics using <code>mlflow.log_metric()</code>.</li> </ol>"},{"location":"exercises/2-intro_to_mlflow_II/","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part II","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n\n\n# Mocked data\nX = np.random.rand(100, 1)  # Independent variable\ny = 2 * X + np.random.randn(100, 1)  # Dependent variable with some noise\n</pre> import numpy as np   # Mocked data X = np.random.rand(100, 1)  # Independent variable y = 2 * X + np.random.randn(100, 1)  # Dependent variable with some noise In\u00a0[4]: Copied! <pre>import matplotlib.pyplot as plt\n\n\n# \ud83d\udc47 Add the relevant code below to plot the data\n</pre> import matplotlib.pyplot as plt   # \ud83d\udc47 Add the relevant code below to plot the data     In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# \ud83d\udc47 Add the relevant code below to split the data into training and testing sets\n</pre> from sklearn.model_selection import train_test_split  # \ud83d\udc47 Add the relevant code below to split the data into training and testing sets     In\u00a0[3]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n# Add code to train the model \ud83d\udc47\n</pre> from sklearn.linear_model import LinearRegression  # Add code to train the model \ud83d\udc47     In\u00a0[9]: Copied! <pre>from sklearn.metrics import mean_squared_error\n\n# Add code to calculate the mean squared error \ud83d\udc47\n</pre> from sklearn.metrics import mean_squared_error  # Add code to calculate the mean squared error \ud83d\udc47     In\u00a0[\u00a0]: Copied! <pre>import mlflow\n\n\nEXPERIMENT_NAME = \"intro-to-mlflow\"\n\n\nexperiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n\n\nwith mlflow.start_run(\n    experiment_id=experiment.experiment_id,\n) as run:\n    \n    # Add code to log the model, the mean squared error, and the model parameters \ud83d\udc47\n\n    pass\n</pre> import mlflow   EXPERIMENT_NAME = \"intro-to-mlflow\"   experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)   with mlflow.start_run(     experiment_id=experiment.experiment_id, ) as run:          # Add code to log the model, the mean squared error, and the model parameters \ud83d\udc47      pass"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-intro-to-mlflow-part-ii","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part II\u00b6","text":"<p>Once we've learned how to log metrics, parameters, and artifacts, we can use MLFlow to track our experiments and compare different models. In this exercise, we'll use some fake data to train a linear regression model. We'll then use MLFlow to track the performance of the model and some relevant information about the training process.</p> <p>In this part we will cover the following topics:</p> <ul> <li>Create some Fake Data</li> <li>Plot the Data using Matplotlib.</li> <li>Split the data into training and testing sets.</li> <li>Train a linear regression model.</li> <li>Compute the accuracy the model.</li> <li>Log the model using MLFlow.</li> <li>Log the accuracy of the model using MLFlow.</li> <li>Log the plotted data using MLFlow.</li> </ul> <p>First we need some data to work with. Let's generate some fake data.</p>"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-i-plot-the-data-using-matplotlib","title":"Exercise I: Plot the Data using Matplotlib\u00b6","text":"<p>\u00bfDo you remember how we can plot data using <code>Matplotlib</code>? Let's do it! \ud83d\ude80:</p> <ol> <li>\ud83d\udc49 We have X (our input) and y (our output). So we can simply plot the data using <code>plt.scatter</code>.</li> <li>\ud83d\udc49 Then we can save the plot using <code>plt.savefig</code>.</li> </ol>"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-ii-split-the-data-into-train-and-test-sets","title":"Exercise II: Split the Data into Train and Test Sets\u00b6","text":"<p>\ud83d\udca1 Remember that we need to split our data into train and test sets. We can use the <code>train_test_split</code> function from <code>sklearn.model_selection</code> to do this. We should store the split into <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, <code>y_test</code>.</p>"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-iii-train-a-linear-regression-model","title":"Exercise III: Train a Linear Regression Model\u00b6","text":"<p>Then, train a linear regression model using the scikit-learn library.</p> <ol> <li>\ud83d\udc49 Initialize the model calling the <code>LinearRegression</code> class.</li> <li>\ud83d\udc49 Train the model using the <code>fit</code> method.</li> </ol>"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-iv-compute-the-accuracy-of-the-model","title":"Exercise IV: Compute the Accuracy of the Model\u00b6","text":"<p>Finally, compute the accuracy of the model using the <code>mean_squared_error</code> function from the <code>sklearn.metrics</code> module.</p> <ol> <li>\ud83d\udc49 Compute the predictions by passing the <code>X_test</code> to the <code>predict</code> method of the model.</li> <li>\ud83d\udc49 Compute the accuracy using the <code>mean_squared_error</code> function and passing the <code>y_test</code> and the <code>predictions</code> as arguments.</li> <li>\ud83d\udc49 Print the accuracy.</li> </ol>"},{"location":"exercises/2-intro_to_mlflow_II/#exercise-v-create-a-run-and-log-the-model-and-metrics","title":"Exercise V: Create a Run and log the model and metrics.\u00b6","text":"<ol> <li>\ud83d\udc49 Think. We've computed the mse of the model. \u00bfWould you log it as a parameter or as a metric?</li> <li>\ud83d\udc49 Think. We've created a plot. \u00bfWhat kind of data is it? \u00bfHow would you log it?</li> <li>\ud83d\udc49 Log the model using the <code>mlflow.sklearn.log_model</code> function.</li> <li>\ud83d\udc49 Extra: Log the signature of the model.</li> </ol>"},{"location":"exercises/3-intro_to_mlflow_III/","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part III","text":"In\u00a0[5]: Copied! <pre>from sklearn import datasets\n\n\n# Download dataset and convert to pandas dataframe\nhousing_dataset = datasets.fetch_california_housing()\nX = housing_dataset.data\ny = housing_dataset.target\n</pre> from sklearn import datasets   # Download dataset and convert to pandas dataframe housing_dataset = datasets.fetch_california_housing() X = housing_dataset.data y = housing_dataset.target In\u00a0[6]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\n\n# \ud83d\udc47 Add the relevant code below to split the data into training and testing sets\n</pre> from sklearn.model_selection import train_test_split   RANDOM_STATE = 42 TEST_SIZE = 0.2  # \ud83d\udc47 Add the relevant code below to split the data into training and testing sets  In\u00a0[7]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n\n# Add code to train the model \ud83d\udc47\n</pre> from sklearn.linear_model import LinearRegression   # Add code to train the model \ud83d\udc47  Out[7]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> In\u00a0[11]: Copied! <pre>from sklearn.metrics import mean_squared_error\n\n\n# Add code to calculate the mean squared error \ud83d\udc47\n</pre> from sklearn.metrics import mean_squared_error   # Add code to calculate the mean squared error \ud83d\udc47  In\u00a0[18]: Copied! <pre>import mlflow\n\n\nEXPERIMENT_NAME = \"intro-to-mlflow\"\nMLFLOW_TRACKING_URI = \"http://localhost:5000\"\n\n# connect to MLFlow\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n\n# Get the experiment\nexperiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n\n# launch a run to log the model\nwith mlflow.start_run(\n    experiment_id=experiment.experiment_id,\n) as run:\n    \n    # Add code to log the model, and the mean squared error \ud83d\udc47\n</pre> import mlflow   EXPERIMENT_NAME = \"intro-to-mlflow\" MLFLOW_TRACKING_URI = \"http://localhost:5000\"  # connect to MLFlow mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  # Get the experiment experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)  # launch a run to log the model with mlflow.start_run(     experiment_id=experiment.experiment_id, ) as run:          # Add code to log the model, and the mean squared error \ud83d\udc47 <pre>/home/sngular/.pyenv/versions/3.11.5/envs/mlops-course/lib/python3.11/site-packages/_distutils_hack/__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n  warnings.warn(\n/home/sngular/.pyenv/versions/3.11.5/envs/mlops-course/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n</pre> In\u00a0[19]: Copied! <pre># register the model for this run\nMODEL_NAME = \"housing-price-predictions\"  # change this to your model name\n\n\n# Compute model path: models stored in a run follow this convention\nmodel_path = f\"runs:/{run_id}/model\"  # fill the `run_id`` variable\n</pre> # register the model for this run MODEL_NAME = \"housing-price-predictions\"  # change this to your model name   # Compute model path: models stored in a run follow this convention model_path = f\"runs:/{run_id}/model\"  # fill the `run_id`` variable <pre>Successfully registered model 'housing-price-predictions'.\n2024/02/29 10:24:08 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: housing-price-predictions, version 1\n</pre> <pre>\u2705 Registered model version: 1!\n</pre> <pre>Created version '1' of model 'housing-price-predictions'.\n</pre> In\u00a0[20]: Copied! <pre>import requests\nimport json\nimport numpy as np\n\n# Define the URL and payload (JSON data)\nurl = 'http://localhost:5001/invocations'\nheaders = {'Content-Type': 'application/json'}\n\n# Create a list representing the (100, 1) vector\ninput_vector = np.random.rand(2, 8).tolist()\n\n# Create a dictionary with the 'inputs' key and the input_vector\npayload = {'inputs': input_vector}\n\n# Convert the payload to JSON format\njson_payload = json.dumps(payload)\n\n# Make a POST request\nresponse = requests.post(url, headers=headers, data=json_payload)\n\n# Check the response\nif response.status_code == 200:\n    print(\"Request successful. Response:\")\n    print(response.text)\nelse:\n    print(f\"Request failed with status code {response.status_code}\")\n    print(response.text)\n</pre> import requests import json import numpy as np  # Define the URL and payload (JSON data) url = 'http://localhost:5001/invocations' headers = {'Content-Type': 'application/json'}  # Create a list representing the (100, 1) vector input_vector = np.random.rand(2, 8).tolist()  # Create a dictionary with the 'inputs' key and the input_vector payload = {'inputs': input_vector}  # Convert the payload to JSON format json_payload = json.dumps(payload)  # Make a POST request response = requests.post(url, headers=headers, data=json_payload)  # Check the response if response.status_code == 200:     print(\"Request successful. Response:\")     print(response.text) else:     print(f\"Request failed with status code {response.status_code}\")     print(response.text) <pre>Request successful. Response:\n{\"predictions\": [-36.83689486654794, -37.10261875122955]}\n</pre>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-intro-to-mlflow-part-iii","title":"\u270d\ufe0f Exercise: Intro to MLFlow - Part III\u00b6","text":"<p>Now that we have loged models into MLFlow it's time to learn how register them and deploy them to a production environment.</p> <ul> <li>Load a regression dataset</li> <li>Train a model</li> <li>Log the model into MLFlow</li> <li>Register the model</li> <li>Stage the model into production/development</li> <li>Deploy the model using MLFlow</li> </ul>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-i-split-the-data-into-train-and-test-sets","title":"Exercise I: Split the Data into Train and Test Sets\u00b6","text":"<p>\ud83d\udca1 Remember that we need to split our data into train and test sets. We can use the <code>train_test_split</code> function from <code>sklearn.model_selection</code> to do this. We should store the split into <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, <code>y_test</code>.</p>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-ii-train-a-linear-regression-model","title":"Exercise II: Train a Linear Regression Model\u00b6","text":"<p>Then, train a linear regression model using the scikit-learn library.</p> <ol> <li>\ud83d\udc49 Initialize the model calling the <code>LinearRegression</code> class.</li> <li>\ud83d\udc49 Train the model using the <code>fit</code> method.</li> </ol>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-iii-compute-the-accuracy-of-the-model","title":"Exercise III: Compute the Accuracy of the Model\u00b6","text":"<p>Finally, compute the accuracy of the model using the <code>mean_squared_error</code> function from the <code>sklearn.metrics</code> module.</p> <ol> <li>\ud83d\udc49 Compute the predictions by passing the <code>X_test</code> to the <code>predict</code> method of the model.</li> <li>\ud83d\udc49 Compute the accuracy using the <code>mean_squared_error</code> function and passing the <code>y_test</code> and the <code>predictions</code> as arguments.</li> <li>\ud83d\udc49 Print the accuracy.</li> </ol>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-iv-create-a-run-and-log-the-model-and-metrics","title":"Exercise IV: Create a Run and log the model and metrics.\u00b6","text":"<ol> <li>\ud83d\udc49 Log the mean squared error metric using <code>mlflow.log_metric</code> function</li> <li>\ud83d\udc49 Log the model using the <code>mlflow.sklearn.log_model</code> function.</li> </ol>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-v-register-the-model","title":"Exercise V: Register the model\u00b6","text":"<p>Registering a model in MLFlow is a way to keep track of the different versions of the same model. Registered models have different versions that track changes in the model and allows</p> <ol> <li>\ud83d\udc49 Get the run ID of the model you want to register using <code>run.info.run_id</code>.</li> <li>\ud83d\udc49 Register the model using the <code>mlflow.register_model</code> function.</li> </ol>"},{"location":"exercises/3-intro_to_mlflow_III/#exercise-vi-deploy-a-model","title":"Exercise VI: Deploy a model\u00b6","text":"<p>Deploying a model is a complex task that involves many steps. MLFlow simplifies this process by providing a set of tools to deploy models to different platforms. In this exercise, we will deploy a model to a local server.</p> <p>First, you need to connect the terminal to the MLFlow Server by setting the <code>MLFLOW_TRACKING_URI</code> environment variable.</p> <pre>export MLFLOW_TRACKING_URI=http://localhost:5000\n</pre> <p>Then, you can deploy the model using the <code>mlflow models serve</code> command in your terminal:</p> <pre>mlflow models serve --model-uri models:/&lt;model_name&gt;/&lt;model_version&gt; --port 5001\n</pre> <p>Where <code>&lt;model_name&gt;</code> is the name of the model and <code>&lt;model_version&gt;</code> is the version of the model you want to deploy. You can find the name and version of the model in the MLFlow UI. Also the <code>--port</code> argument is the port where the server will be running. It's important to choose a port different than the <code>5000</code> port where the MLFlow server is running.</p>"},{"location":"exercises/3-intro_to_mlflow_III/#bonus-make-a-request-to-the-model","title":"BONUS: Make a request to the model\u00b6","text":"<p>Finally, make a request to the model using the <code>requests</code> library. You can use the following code to make a request to the model:</p>"},{"location":"exercises/4-intro_to_mlflow_IV/","title":"Exercise: Call a Model API","text":"In\u00a0[1]: Copied! <pre>from sklearn import datasets\n\n\n# Download dataset and convert to pandas dataframe\nhousing_dataset = datasets.fetch_california_housing(as_frame=True)\nX = housing_dataset.data\n</pre> from sklearn import datasets   # Download dataset and convert to pandas dataframe housing_dataset = datasets.fetch_california_housing(as_frame=True) X = housing_dataset.data In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\n\n# \ud83d\udc47 write the code here\n</pre> from sklearn.model_selection import train_test_split   RANDOM_STATE = 42 TEST_SIZE = 0.2  # \ud83d\udc47 write the code here In\u00a0[\u00a0]: Copied! <pre># \ud83d\udc47 write the code here\n</pre> # \ud83d\udc47 write the code here  In\u00a0[3]: Copied! <pre>import json\n\n# \ud83d\udc47 write the code here\n</pre> import json  # \ud83d\udc47 write the code here  In\u00a0[\u00a0]: Copied! <pre>import requests\n\n# \ud83d\udc47 write the code here\n</pre> import requests  # \ud83d\udc47 write the code here  In\u00a0[\u00a0]: Copied! <pre># \ud83d\udc47 write the code here\n</pre> # \ud83d\udc47 write the code here"},{"location":"exercises/4-intro_to_mlflow_IV/#exercise-call-a-model-api","title":"Exercise: Call a Model API\u00b6","text":"<p>In this exercise, you will call a MLFlow model API to make predictions on a dataset.</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#1-deploy-a-model","title":"1. Deploy a Model\u00b6","text":"<p>Remember we first need to deploy the model to be able to call it.</p> <ol> <li>First, you need to connect the terminal to the MLFlow Server by setting the <code>MLFLOW_TRACKING_URI</code> environment variable.</li> </ol> <pre>export MLFLOW_TRACKING_URI=http://localhost:5000\n</pre> <ol> <li>Then, you can deploy the model using the <code>mlflow models serve</code> command in your terminal:</li> </ol> <pre>mlflow models serve --model-uri models:/&lt;&gt;/&lt;model_version&gt; --port 5001 --env-manager conda\n</pre> <p>Where <code>&lt;model_name&gt;</code> is the name of the model and <code>&lt;model_version&gt;</code> is the version of the model you want to deploy. You can find the name and version of the model in the MLFlow UI. Also the <code>--port</code> argument is the port where the server will be running. It's important to choose a port different than the <code>5000</code> port where the MLFlow server is running. The <code>--env-manager</code> argument is the environment manager that MLFlow will use to run the model. In this case, we are using <code>conda</code> but you can use <code>pip</code> or <code>docker</code> as well.</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#2-get-some-data-to-make-predictions","title":"2. Get some data to make predictions.\u00b6","text":"<p>You can use the following code to get some data from the <code>california_housing</code> dataset:</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#3-split-the-data-to-get-the-test-set","title":"3. Split the data to get the test set\u00b6","text":"<p>\ud83d\udca1 Just as an example we can use the test set to make predictions. We can use the <code>train_test_split</code> function from <code>sklearn.model_selection</code> to do this. We should store the split into <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, <code>y_test</code>.</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#4-prepare-the-data-to-be-sent","title":"4. Prepare the data to be sent\u00b6","text":"<p>We need to prepare the data to be sent to the model API.</p> <ul> <li>\ud83d\udc49 Create the headers</li> <li>\ud83d\udc49 Create the body</li> </ul>"},{"location":"exercises/4-intro_to_mlflow_IV/#5-convert-the-data-to-json","title":"5. Convert the data to JSON\u00b6","text":"<p>We need to convert the data to JSON to be able to send it to the model API. You can use the <code>json.dumps</code> function from the <code>json</code> module to do this.</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#6-send-the-request","title":"6. Send the request\u00b6","text":"<p>You can use the <code>requests</code> library to send the request to the model API. You can use the <code>requests.post</code> function to do this.</p>"},{"location":"exercises/4-intro_to_mlflow_IV/#7-show-the-response","title":"7. Show the Response\u00b6","text":"<p>You can use the <code>response.json()</code> method to show the response from the model API. Also check the <code>response.status_code</code> to see if the request was successful.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/","title":"MLFlow for Kaggle I - Loan Approval Prediction","text":"<p>We then need to get an API TOKEN from kaggle. To do this we need to go to the kaggle website, click on the profile icon and then click on \"My Account\". In the \"API\" section we can click on \"Create New API Token\" to download the token. We then need to move the token to the <code>~/.kaggle</code> folder and rename it to <code>kaggle.json</code>.</p> <p></p> <p>Once we've join the competition we can download the data using the following commands:</p> <pre>pip install kaggle\nkaggle competitions download -c playground-series-s4e10\n</pre> <p>You also have the option to download the data manually by going to <code>Data</code> &gt; <code>Download Data</code></p> <p></p> <p>Once we've downloaded the data we are going to save it into <code>data/loan_prediciton</code>. We can see there are three files:</p> <ul> <li><code>train.csv</code>: The training data</li> <li><code>test.csv</code>: The test data</li> <li><code>sample_submission.csv</code>: The sample submission file</li> </ul> <p>\u2139\ufe0f Remember that <code>test.csv</code> does not have the target column, we will use it to evaluate our model against the kaggle platform.</p> In\u00a0[1]: Copied! <pre>\n</pre> Out[1]: id person_age person_income person_home_ownership person_emp_length loan_intent loan_grade loan_amnt loan_int_rate loan_percent_income cb_person_default_on_file cb_person_cred_hist_length loan_status train 0 0 37 35000 RENT 0.0 EDUCATION B 6000 11.49 0.17 N 14 0.0 1 1 22 56000 OWN 6.0 MEDICAL C 4000 13.35 0.07 N 2 0.0 2 2 29 28800 OWN 8.0 PERSONAL A 6000 8.90 0.21 N 10 0.0 3 3 30 70000 RENT 14.0 VENTURE B 12000 11.11 0.17 N 5 0.0 4 4 22 60000 RENT 2.0 MEDICAL A 6000 6.92 0.10 N 3 0.0 In\u00a0[2]: Copied! <pre>\n</pre> <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 58645 entries, 0 to 58644\nData columns (total 13 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   id                          58645 non-null  int64  \n 1   person_age                  58645 non-null  int64  \n 2   person_income               58645 non-null  int64  \n 3   person_home_ownership       58645 non-null  object \n 4   person_emp_length           58645 non-null  float64\n 5   loan_intent                 58645 non-null  object \n 6   loan_grade                  58645 non-null  object \n 7   loan_amnt                   58645 non-null  int64  \n 8   loan_int_rate               58645 non-null  float64\n 9   loan_percent_income         58645 non-null  float64\n 10  cb_person_default_on_file   58645 non-null  object \n 11  cb_person_cred_hist_length  58645 non-null  int64  \n 12  loan_status                 58645 non-null  int64  \ndtypes: float64(3), int64(6), object(4)\nmemory usage: 5.8+ MB\n</pre> In\u00a0[3]: Copied! <pre>\n</pre> <pre>Missing values: id                            0\nperson_age                    0\nperson_income                 0\nperson_home_ownership         0\nperson_emp_length             0\nloan_intent                   0\nloan_grade                    0\nloan_amnt                     0\nloan_int_rate                 0\nloan_percent_income           0\ncb_person_default_on_file     0\ncb_person_cred_hist_length    0\nloan_status                   0\ndtype: int64\nDuplicated values: 0\n</pre> In\u00a0[4]: Copied! <pre>\n</pre> Out[4]: id person_age person_income person_home_ownership person_emp_length loan_intent loan_grade loan_amnt loan_int_rate loan_percent_income cb_person_default_on_file cb_person_cred_hist_length loan_status train 0 0 37 35000 3 0.0 1 1 6000 11.49 0.17 0 14 0.0 1 1 22 56000 2 6.0 3 2 4000 13.35 0.07 0 2 0.0 2 2 29 28800 2 8.0 4 0 6000 8.90 0.21 0 10 0.0 3 3 30 70000 3 14.0 5 1 12000 11.11 0.17 0 5 0.0 4 4 22 60000 3 2.0 3 0 6000 6.92 0.10 0 3 0.0 In\u00a0[5]: Copied! <pre>\n</pre> Out[5]: <pre>Text(0.5, 1.0, 'Correlation Heatmap')</pre> In\u00a0[22]: Copied! <pre>\n</pre> In\u00a0[23]: Copied! <pre>\n</pre> Out[23]: <pre>RandomForestClassifier(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriFitted<pre>RandomForestClassifier(random_state=42)</pre> In\u00a0[19]: Copied! <pre>\n</pre> <pre>ROC AUC: 0.9347986903855718\n</pre> In\u00a0[9]: Copied! <pre>\n</pre> Out[9]: id loan_status 0 58645 1.0 1 58646 0.0 2 58647 1.0 3 58648 0.0 4 58649 0.0 In\u00a0[10]: Copied! <pre>\n</pre> Out[10]: <pre>&lt;Experiment: artifact_location='mlflow-artifacts:/461999357438845049', creation_time=1729443470096, experiment_id='461999357438845049', last_update_time=1729443470096, lifecycle_stage='active', name='Loan Prediction', tags={}&gt;</pre> In\u00a0[11]: Copied! <pre>\n</pre> <pre>2024/10/22 15:00:26 INFO mlflow.tracking._tracking_service.client: \ud83c\udfc3 View run industrious-doe-341 at: http://localhost:5000/#/experiments/461999357438845049/runs/7776305ffd064f84b1d1f00fbda239f2.\n2024/10/22 15:00:26 INFO mlflow.tracking._tracking_service.client: \ud83e\uddea View experiment at: http://localhost:5000/#/experiments/461999357438845049.\n</pre> In\u00a0[24]: Copied! <pre>runs = mlflow.search_runs(order_by=['metrics.auc DESC'])\nbest_run_id = runs.iloc[0][\"run_id\"]\nbest_run_name = runs.iloc[0][\"tags.mlflow.runName\"]\nf\"Best run: {best_run_name} (run_id: {best_run_id})\"\n</pre> runs = mlflow.search_runs(order_by=['metrics.auc DESC']) best_run_id = runs.iloc[0][\"run_id\"] best_run_name = runs.iloc[0][\"tags.mlflow.runName\"] f\"Best run: {best_run_name} (run_id: {best_run_id})\" Out[24]: <pre>'Best run: industrious-doe-341 (run_id: 7776305ffd064f84b1d1f00fbda239f2)'</pre> In\u00a0[13]: Copied! <pre>from pathlib import Path\nfrom mlflow import MlflowClient\n\n\nclient = MlflowClient()\nsubmission_filename = Path(SUBMISSION_CSV_PATH).name\nsubmision_csv_path = client.download_artifacts(best_run_id, submission_filename, DATA_PATH)\n</pre> from pathlib import Path from mlflow import MlflowClient   client = MlflowClient() submission_filename = Path(SUBMISSION_CSV_PATH).name submision_csv_path = client.download_artifacts(best_run_id, submission_filename, DATA_PATH) <pre>Downloading artifacts:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre>import kaggle\n\n\ncompetition = \"playground-series-s4e10\"\nmessage = f\"Loan Prediction for run ID {best_run_name}\"\nkaggle.api.competition_submit(submision_csv_path, message, competition)\n</pre> import kaggle   competition = \"playground-series-s4e10\" message = f\"Loan Prediction for run ID {best_run_name}\" kaggle.api.competition_submit(submision_csv_path, message, competition) <pre>Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/codespace/.kaggle/kaggle.json'\n</pre> <pre>  0%|          | 0.00/382k [00:00&lt;?, ?B/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382k/382k [00:00&lt;00:00, 498kB/s]\n</pre> Out[14]: <pre>Successfully submitted to Loan Approval Prediction</pre> <ol> <li>Log the Correlation Heatmap to MLflow</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Model Tracking: log the model parameters to MLFlow</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Model Training: try another model from the <code>sklearn</code> library log the results and compare them in MLFLow</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Model Training: try another models from other libraries like <code>xgboost</code>, <code>lightgbm</code>, <code>catboost</code>... and compare the results</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Model Evaluation: try another metrics (which metrics should we use for a classification problem?) and log them to MLFlow</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Think about the train-test-validation split. How can we improve our model generalization?</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Model Evaluation: Think about the evaluation we are doing. Is it correct? How can we improve it?</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Take a look into the Kaggle Code Section, did you find any interesting notebooks? Optional: Try to implement some of the ideas in this notebook.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <ol> <li>Optional: Upload this notebook to the Code Section of this Kaggle competition (show a picture of the notebook in the comments)</li> </ol>"},{"location":"exercises/5-mlflow_for_kaggle_I/#mlflow-for-kaggle-i-loan-approval-prediction","title":"MLFlow for Kaggle I - Loan Approval Prediction\u00b6","text":"<p>In this notebook we will use the Loan Approval dataset to demonstrate how to build a machine learning pipeline using the MLFlow library. We will use the following steps:</p> <ol> <li>Data Collection</li> <li>Data Preprocessing</li> <li>Train, Test, Validation Split</li> <li>Model Training</li> <li>Model Evaluation</li> <li>Model Tracking</li> <li>Kaggle Submission</li> <li>BONUS: Exercises</li> </ol>"},{"location":"exercises/5-mlflow_for_kaggle_I/#1-data-collection","title":"1. Data Collection\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#loan-approval-prediction","title":"Loan Approval Prediction\u00b6","text":"<p>We are going to take the data form kaggle, so we need to install the kaggle library and download the data. Here is the link to the Loan Approval Prediction: https://www.kaggle.com/competitions/playground-series-s4e10/overview. In order to start this competition we need to manually join the competition to accept the terms and conditions.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#explore-the-data","title":"Explore the data\u00b6","text":"<p>Let's load the data and take a look at the first few rows.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#2-data-preprocessing","title":"2. Data Preprocessing\u00b6","text":"<p>Remember that we need to preprocess the data before training the model. We will do the following steps:</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#encode-cateforical-variables","title":"Encode Cateforical Variables\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#data-correlation","title":"Data Correlation\u00b6","text":"<p>Let's take a look at the correlation between the features and the target variable</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#3-train-test-validation-split","title":"3. Train, Test, Validation split\u00b6","text":"<p>We now split the the dataframe into training and validation sets. We will use the training set to train the model and the validation set to evaluate the model. Finally, we will use the test set to generate the predictions and send them to the kaggle platform.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#4-model-training","title":"4. Model Training\u00b6","text":"<p>Once we have preprocessed the data we can train the model.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#5-model-evaluation","title":"5. Model Evaluation\u00b6","text":"<p>This competition uses the Area Under the Curve (AUC) as the evaluation metric.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#generate-submission-file","title":"Generate submission file\u00b6","text":"<p>Looking at the <code>submission_sample.csv</code> file we see that it hmut have two columns: <code>id</code> and <code>loan_status</code>. The <code>id</code> column must have the same values as the test set and the <code>loan_status</code> column must have the predictions.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#6-model-tracking","title":"6. Model Tracking\u00b6","text":"<p>Once we have trained the model we can use the MLFlow library to track the model performance. We can log the AUC, the metrics, the submission file...</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#connect-to-mlflow","title":"Connect to MLFlow\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#submit-to-mlflow","title":"Submit to MLFlow\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#7-kaggle-submission","title":"7. Kaggle Submission\u00b6","text":"<p>Finally we can submit the predictions to the kaggle platfrom. To do so we are going to retrieve the submission CSV from the best MLFlow run and submit it to the kaggle platform.</p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#get-the-best-run","title":"Get the best run\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#download-the-submission-file","title":"Download the submission file\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#submit-to-kaggle","title":"Submit to Kaggle\u00b6","text":"<p>You could upload the results manually by going to the kaggle website and clicking on <code>Submit Predictions</code>. You can then upload the file and submit it. Or you can use the kaggle API to submit the file:</p> <p></p>"},{"location":"exercises/5-mlflow_for_kaggle_I/#my-submission","title":"\ud83c\udf89 My Submission!:\u00b6","text":""},{"location":"exercises/5-mlflow_for_kaggle_I/#bonus-exercises","title":"Bonus: Exercises\u00b6","text":"<p>Now improve this notebook by doing the following exercises:</p>"},{"location":"exercises/creating_a_docker_image/","title":"Introduction to Docker - Part 2: Building Your Own Image","text":"<p>Welcome to the enhanced second part of our Docker introduction! In this section, we'll guide you through the process of building your own Docker image step by step using a simple example with a Python application. We'll use FastAPI, a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.</p>"},{"location":"exercises/creating_a_docker_image/#step-1-create-a-fastapi-application","title":"Step 1: Create a FastAPI Application","text":"<p>Create a file named <code>main.py</code> within the directory:</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"Docker! This is a FastAPI application.\"}\n</code></pre>"},{"location":"exercises/creating_a_docker_image/#step-2-create-a-dockerfile","title":"Step 2: Create a Dockerfile","text":"<ol> <li> <p>First create a file named: <code>Dockerfile</code></p> <pre><code>touch Dockerfile\n</code></pre> </li> <li> <p>Define the Base Image: Start by specifying the base image for your Dockerfile. We'll use an official Python image, specifically version 3.9-slim, to keep it lightweight. Inside the Dockerfile, add the following line:</p> <pre><code># Use an official Python image as the base image\nFROM python:3.9-slim\n</code></pre> </li> <li> <p>Install FastAPI and Uvicorn: Install the necessary Python packages for your FastAPI application. In this case, we need FastAPI itself and Uvicorn, which is the ASGI server FastAPI uses. Add the following line to the Dockerfile:</p> <pre><code># Install FastAPI and Uvicorn\nRUN pip install fastapi uvicorn\n</code></pre> </li> <li> <p>Copy Application Code: Copy the entire content of your local directory (where the Dockerfile is located) into the container's working directory. Add the following line to the Dockerfile:</p> <pre><code># Copy the application code to the container\nCOPY main.py .\n</code></pre> </li> <li> <p>Define the Command to Run the Application: Specify the command to run your FastAPI application using Uvicorn. The <code>--host 0.0.0.0</code> makes it accessible from outside the container. The <code>--port 8000</code> makes the API available on port 8000. Add the following line to the Dockerfile:</p> <pre><code># Command to run the application using Uvicorn\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> </li> <li> <p>Save and Close the Dockerfile: Save your changes and close the text editor.</p> </li> </ol>"},{"location":"exercises/creating_a_docker_image/#step-3-build-the-docker-image","title":"Step 3: Build the Docker Image","text":"<p>Open a terminal in the directory containing your Dockerfile and application code. Run the following command to build the Docker image:</p> <pre><code>docker build -t my-fastapi-app .\n</code></pre>"},{"location":"exercises/creating_a_docker_image/#step-4-run-the-docker-container","title":"Step 4: Run the Docker Container","text":"<p>Once the image is built, you can run a container based on it with the following command:</p> <pre><code>docker run -p 8000:8000 my-fastapi-app\n</code></pre> <p>Remember run parameters</p> <ul> <li><code>-p 8000:8000</code>: Maps port 8000 on the host to port 8000 on the container.</li> <li><code>my-fastapi-app</code>: The name of the Docker image to run.</li> </ul> <p>Visit http://localhost:8000 in your web browser to interact with your FastAPI application.</p> <p>Congratulations! You've successfully built a Docker image for a FastAPI application and run it in a container. Feel free to explore more features of FastAPI and Docker as you continue your learning journey.</p>"},{"location":"fundamentals/","title":"Fake News Detection Project","text":"<p>Welcome to our Fake News Detection Project, where we embark on a journey to build a robust system that can determine the authenticity of news articles. In a world inundated with information, the ability to distinguish between real and fake news is crucial.</p> <p>Real Project from the ground up</p> <p>We'll begin by creating a solid development environment, leverage cutting-edge Natural Language Processing (NLP) techniques to convert news articles into meaningful embeddings, and train a machine learning model capable of making accurate predictions. </p> <p></p> <p>Throughout this project, we will embrace best practices in software development, version control, and containerization to ensure reproducibility and ease of deployment. Our goal is to not only develop a powerful tool but also to learn and grow as we build a project from the ground up. Let's get started on this exciting journey towards a more informed and discerning world!</p>"},{"location":"fundamentals/#project-overview","title":"Project Overview","text":"<ol> <li>Create a virtual environment</li> <li>Start a new project using Poetry</li> <li>Manage the repository using GIT</li> <li>Download dataset from Kaggle</li> <li>Docker-Compose: Create environment with MLFlow</li> <li>Use a sentence transformers model to transform news to vectors.</li> <li>Retrieve the embeddings and Train a single model on those embeddings and register to MLFlow</li> <li>Docker-Compose: Add a service that takes the latest model from MLFlow and crates an API</li> <li>Create a Streamlit App that calls the API and returns a response</li> <li>Dockerize Streamlit App and add it to the docker compose</li> </ol>"},{"location":"fundamentals/intro_to_api/","title":"Introduction to APIs","text":"<p>Imagine you're at a restaurant with a menu full of delicious dishes to choose from. You decide what you want and tell the waiter, who then goes to the kitchen to tell the chef exactly what to prepare for you. After a bit of wait, the waiter returns with your order exactly as you requested. In this scenario, think of the menu as a list of services, you as the user, the waiter as the messenger, and the kitchen as the system that prepares your request. This process is quite similar to how an API, or Application Programming Interface, works in the digital world.</p> <p>An API is essentially a set of rules, protocols, and tools for building software and applications. It acts as an intermediary that allows two different programs to communicate with each other. Just like the waiter in our restaurant example, an API takes a request from one system (you ordering food), translates it into a format that the other system (the kitchen) can understand, and then delivers the response back to the requesting system (the waiter bringing your food).</p>"},{"location":"fundamentals/intro_to_api/#how-does-an-api-work","title":"How Does an API Work?","text":"<p>When you use an application on your smartphone or computer, that application connects to the Internet and sends data to a server. The server then retrieves that data, interprets it, performs the necessary actions, and sends it back to your phone or computer. The application then interprets that data and presents you with the information you wanted in a readable way. All this back-and-forth is done through APIs.</p> <p></p>"},{"location":"fundamentals/intro_to_api/#parts-of-an-api","title":"Parts of an API","text":"<p>An API is not just a single element but rather a set of components that work together to enable seamless communication between different software systems. Here are the key parts of an API:</p> <p></p>"},{"location":"fundamentals/intro_to_api/#1-endpoints","title":"1. Endpoints","text":"<p>An endpoint is a specific address (URL) on the web where an API can be accessed by a client application. It's like a specific location in the kitchen where orders are taken and served. Each endpoint is associated with a specific function or resource that the API can provide, such as retrieving user information or posting a message.</p>"},{"location":"fundamentals/intro_to_api/#2-methods","title":"2. Methods","text":"<p>API methods (or HTTP methods) define what action you want to perform on an endpoint, such as GET to retrieve data, POST to send data to the server, PUT to update data, and DELETE to remove data. These methods correspond to the different types of requests you can make, similar to how you can order food, request a bill, or cancel an order in a restaurant.</p> <p>Understanding HTTP Methods: GET, POST, PUT, DELETE</p> <p>HTTP methods define the action to be performed on resources identified by URLs. Understanding these methods is crucial for API interaction.</p> <p>\ud83d\udc47 Click on each method to understand their use</p> GETPOSTPUTDELETE <p>The <code>GET</code> method requests a representation of the specified resource. Requests using <code>GET</code> should only retrieve data.</p> <p>Example Use: Fetching a user's profile information.</p> <pre><code>GET /users/12345\nUrl: example.com\n</code></pre> <p>The <code>POST</code> method submits an entity to the specified resource, often causing a change in state or side effects on the server.</p> <p>Example Use: Creating a new user account.</p> <pre><code>POST /users\nUrl: example.com\nContent-Type: application/json\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\"\n}\n</code></pre> <p>The <code>PUT</code> method replaces all current representations of the target resource with the request payload.</p> <p>Example Use: Updating user's profile information.</p> <pre><code>PUT /users/12345\nUrl: example.com\nContent-Type: application/json\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@example.com\"\n}\n</code></pre> <p>The <code>DELETE</code> method deletes the specified resource.</p> <p>Example Use: Removing a user's account.</p> <pre><code>DELETE /users/12345\nUrl: example.com\n</code></pre>"},{"location":"fundamentals/intro_to_api/#3-request-and-response","title":"3. Request and Response","text":"<ul> <li> <p>Request: When a client application wants to communicate with an API, it sends a request. This request includes the endpoint it\u2019s trying to reach, the method it\u2019s using, and any data it needs to send along. It\u2019s like giving your order to the waiter.</p> </li> <li> <p>Response: After the API processes the request, it sends back a response. This response can include the data requested, confirmation of a new record created, or a status message indicating the success or failure of the request. This is akin to the waiter bringing your order to the table.</p> </li> </ul>"},{"location":"fundamentals/intro_to_api/#4-headers","title":"4. Headers","text":"<p>Headers in an API request and response provide additional information about the request or the data being sent. For example, headers can specify the type of data being sent (JSON, XML), authentication details, or instructions for caching the data. Think of headers as the special instructions you might give when placing an order, like asking for a dish to be extra spicy or to exclude a certain ingredient.</p>"},{"location":"fundamentals/intro_to_api/#5-payload-body","title":"5. Payload (Body)","text":"<p>The payload, or body, of an API request is the actual data you send with the request. This is only present in methods that send data to the server, like POST or PUT. In our restaurant analogy, the payload is similar to the specific details of your order, including the dish you want and any special requests.</p>"},{"location":"fundamentals/intro_to_api/#6-status-codes","title":"6. Status Codes","text":"<p>Status codes are part of the API response and tell the client application whether the request was successful, and if not, why. Common status codes include 200 (OK), 404 (Not Found), and 500 (Internal Server Error). These codes help the client understand the outcome of their request, much like getting immediate feedback from the waiter on whether your order can be fulfilled.</p> <p>Understanding HTTP Status Code Families Through a Library Visit</p> <p>\ud83d\udc47 Click to explore</p> 100s: Informational Responses200s: Success300s: Redirections400s: Client Errors500s: Server Errors <p>Imagine walking into a library and asking if a new book has arrived. The librarian nods and goes to check. This is like the 100s status codes, where the server is saying, \"I've received your request and am continuing to process it.\"</p> <p>You ask for a specific book, and the librarian hands it to you with a smile. This is akin to the 200s status codes, signifying that your request was successfully received, understood, and accepted.</p> <p>If you're told the book you're looking for is in another section or even another branch, and you're directed there. The 300s status codes are similar, indicating that more actions need to be taken by the client to fulfill the request.</p> <p>Suppose you request a book title that doesn't exist. The librarian informs you there's no such book. This mirrors the 400s status codes, where the request cannot be fulfilled due to apparent client error (e.g., malformed request syntax, invalid request message framing, or deceptive request routing).</p> <p>Finally, imagine if the library's computer system suddenly goes down while your request is being processed. This situation is represented by the 500s status codes, indicating that the server encountered an unexpected condition that prevented it from fulfilling your request.</p> <p>Together, these components allow APIs to function as effective intermediaries between different software systems, translating requests into actions and responses into data that can be understood and used by the client application. Understanding these parts can help developers more effectively utilize APIs in their projects.</p>"},{"location":"fundamentals/intro_to_api/#calling-an-api","title":"Calling an API","text":"<p>Calling an API using python is simple. You can use the <code>requests</code> library to send a request to an API endpoint and receive a response. Here's an example of how you can use the <code>requests</code> library to call an API:</p> <pre><code>import requests\n\nendpoint = 'https://jsonplaceholder.typicode.com/todos/1'\n\nresponse = requests.get(endpoint)\n</code></pre>"},{"location":"fundamentals/intro_to_api/#types-of-apis","title":"Types of APIs","text":"<p>There are several types of APIs, including:</p> <ul> <li> <p>Web APIs: These are APIs on the internet that allow applications to communicate with each other over the web. For example, when you use a social media app to share a news article, the app uses an API to send the article to the server and then display a confirmation to you.</p> </li> <li> <p>Operating System APIs: These allow applications to use functions of the operating system, like displaying something on the screen or writing to a file.</p> </li> <li> <p>Database APIs: These let applications communicate with databases to fetch, create, update, or delete data.</p> </li> <li> <p>Remote APIs: These are used for communications between devices over a network.</p> </li> </ul>"},{"location":"fundamentals/intro_to_api/#why-are-apis-important","title":"Why are APIs Important?","text":"<p>APIs are crucial because they enable the integration between different software systems, allowing them to work together. This can make it possible to:</p> <ul> <li>Automate tasks and improve efficiency.</li> <li>Enhance user experience by integrating functionality from various services. For instance, a weather application pulling data from a remote weather API to provide users with the latest weather updates.</li> <li>Securely share data between different systems.</li> </ul> <p>In summary, APIs are like the unsung heroes of the digital world, working behind the scenes to connect the technology we use every day. They make it possible for the software to interact seamlessly, automate tasks, and provide the functionalities that users demand in their applications. Without APIs, our experience with technology would be much more fragmented and less efficient.</p>"},{"location":"fundamentals/intro_to_docker/","title":"Introduction to Docker","text":"<p>Docker is a powerful platform that simplifies the process of developing, shipping, and running applications. It utilizes containerization technology to package an application and its dependencies into a single, lightweight unit known as a container.</p> <p></p>"},{"location":"fundamentals/intro_to_docker/#what-is-docker","title":"What is Docker?","text":"<p>In simple terms, Docker allows you to encapsulate your application and its dependencies into a standardized unit, ensuring that it runs consistently across various environments. This is achieved through the use of containers, which are isolated and portable environments that can run seamlessly on any machine supporting Docker.</p> <p></p>"},{"location":"fundamentals/intro_to_docker/#problems-docker-solves","title":"Problems Docker Solves","text":""},{"location":"fundamentals/intro_to_docker/#it-works-on-my-machine","title":"\"It works on my machine\"","text":"<p>One common issue in software development is the disparity between development and production environments. With Docker, you can eliminate the infamous \"It works on my machine\" problem. Containers ensure that your application runs consistently across different stages of development, reducing compatibility issues and streamlining the deployment process.</p>"},{"location":"fundamentals/intro_to_docker/#scalability","title":"Scalability","text":"<p>Docker also addresses scalability challenges. Containers can be easily replicated and deployed across multiple machines, providing a scalable and efficient solution. This allows your application to handle increased workloads effortlessly, making Docker an excellent choice for both small and large-scale projects.</p> <p></p>"},{"location":"fundamentals/intro_to_docker/#how-docker-works","title":"How Docker Works","text":"<p>The Docker workflow involves three main components: Dockerfile, Image, and Container.</p> <ol> <li> <p>Dockerfile: A Dockerfile is a script that contains instructions for building a Docker image. It specifies the base image, adds dependencies, configures the environment, and sets up the application.</p> </li> <li> <p>Image: An image is a lightweight, standalone, and executable package that includes everything needed to run an application. It is created from a Dockerfile and serves as a template for containers.</p> </li> <li> <p>Container: A container is an instance of a Docker image. It encapsulates the application and its dependencies, running in isolation from the host system. Containers are portable and can be easily moved between different environments.</p> </li> </ol> <p></p>"},{"location":"fundamentals/intro_to_docker/#docker-networks-and-ports","title":"Docker Networks and Ports","text":"<p>Docker provides a networking capability that enables communication between containers and the outside world. Each container can be assigned its own network or share one with others, facilitating seamless interaction. Ports can be mapped between the host and containers, allowing external access to services within the containers.</p> <p></p>"},{"location":"fundamentals/intro_to_docker/#docker-commands","title":"Docker Commands","text":"<p>Docker commands play a crucial role in building, managing, and deploying containers. Two fundamental commands are <code>docker build</code> and <code>docker run</code>, each with various parameters that allow customization and flexibility in containerization workflows.</p>"},{"location":"fundamentals/intro_to_docker/#docker-run-command","title":"<code>docker run</code> Command","text":"<p>The <code>docker run</code> command is used to create and start a container from a Docker image. Here are some key parameters:</p> Parameter Description <code>-d, --detach</code> Runs the container in the background. <code>--name</code> Assigns a name to the container. <code>-p, --publish</code> Maps host ports to container ports in the format <code>hostPort:containerPort</code>. <code>--network</code> Connects the container to a specific network. <code>-e, --env</code> Sets environment variables inside the container. <code>--volume</code> Mounts a volume from the host to the container. <code>--restart</code> Sets the container restart policy (<code>always</code>, <code>unless-stopped</code>, <code>on-failure</code>, etc.). <code>--link</code> Deprecated. Connects containers in a user-defined network. <p>Example:</p> <pre><code>docker run -d \\\n--name myapp-container \\\n--publish 8080:80 \\\n--network my-network \n-e MY_ENV=my-value \\\nmyapp:latest\n</code></pre> <p>These commands and parameters provide a foundation for building and running Docker containers. Understanding their usage is essential for efficiently managing your containerized applications.</p> <p>Try Docker run</p> <p>A simple docker you can try is the \"cow-say\" \ud83d\udc2e image. Run the following command to see a cow saying \"Hello, World!\".</p> <pre><code>docker run rancher/cowsay Hello, World!\n</code></pre> <p>\ud83d\udc2e result:</p> <pre><code>_______________ \n&lt; Hello, World! &gt;\n--------------- \n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>Run a MLFlow server with Docker</p> <p>You can run a MLFlow server with Docker using the following command:</p> <pre><code>docker run --network host ghcr.io/mlflow/mlflow:v2.17.0 mlflow server\n</code></pre> <p>Then go to <code>http://localhost:5000</code> to access the MLFlow server.</p> <p>Deploy a MLFlow model with Docker</p> <p>You can deploy a MLFlow model with Docker using the following command:</p> <pre><code>docker run \\\n--net host \\\n--env MLFLOW_TRACKING_URI=http://localhost:5000 \\\nghcr.io/mlflow/mlflow:v2.17.0 \\\nmlflow models serve \\\n--model-uri models:/&lt;model_name&gt;/&lt;model_version&gt; \\\n--port 5001 \\\n--env-manager local\n</code></pre> <p>Then go to <code>http://localhost:1234</code> to access the MLFlow model.</p>"},{"location":"fundamentals/intro_to_docker/#docker-build-command","title":"<code>docker build</code> Command","text":"<p>The <code>docker build</code> command is used to create a Docker image from a Dockerfile. Below is an explanation of its essential parameters:</p> Parameter Description <code>-t, --tag</code> Assigns a name and optionally a tag to the image (format: <code>name:tag</code>). <code>-f, --file</code> Specifies the path to the Dockerfile. Default is <code>./Dockerfile</code>. <code>--build-arg</code> Sets build-time variables in the format <code>key=value</code>. <code>--no-cache</code> Forces Docker to build the image from scratch, ignoring the cache. <code>--rm</code> Removes intermediate containers after a successful build. <code>--target</code> Allows specifying a build stage to stop at when building a multi-stage Dockerfile. <p>Example:</p> <pre><code>docker build \\\n-t myapp:latest \\\n-f ./path/to/Dockerfile .\n</code></pre>"},{"location":"fundamentals/intro_to_docker/#docker-hub","title":"Docker Hub","text":"<p>Docker Hub is a cloud-based repository that hosts Docker images. It provides a centralized platform for sharing, storing, and managing images, making it easy to access and distribute containerized applications. Docker Hub also supports automated builds, webhooks, and integration with other services, enhancing the development and deployment process.</p>"},{"location":"fundamentals/intro_to_git/","title":"Introduction to GIT","text":"<p>GIT is a powerful version control system that helps you track changes in your code and collaborate with others. This tutorial is aimed at beginners and will cover the basics of GIT.</p> <p>How to install GIT</p> <p>Before you can use GIT, you need to install it on your computer. Visit the official GIT website and download the installer for your operating system. Follow the installation instructions for your platform.</p> <p>Configuring GIT</p> <p>After installing GIT, you need to configure your name and email address. Open a terminal or command prompt and run the following commands, replacing \"Your Name\" and \"your.email@example.com\" with your actual name and email:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email your.email@example.com\n</code></pre>"},{"location":"fundamentals/intro_to_git/#the-basic-git-workflow","title":"The Basic GIT Workflow","text":"<p>Develop a new feature aside from the project:</p> <ol> <li>Create a branch</li> <li>Making changes</li> <li>Staging changes</li> <li>Commit changes</li> <li>Push changes from local repository to remote repository</li> </ol> <p>Integrate the new feature into the project.</p> <ol> <li>Create a Pull Request (Github) or Merge Request (Gitlab) remotely</li> <li>Merge Changes remotely</li> <li>Remove Branch remotely</li> <li>Pull Changes from remote repository to local repository</li> <li>Remove Branch locally</li> </ol> <p></p>"},{"location":"fundamentals/intro_to_git/#creating-a-git-repository","title":"Creating a GIT Repository","text":"<p>A GIT repository is a place where you can store your project's code. Navigate to the directory where your project is located in the terminal and run the following command to initialize a new GIT repository:</p> <p>Start A Repository</p> <pre><code>git init\n</code></pre>"},{"location":"fundamentals/intro_to_git/#adding-files-to-the-repository","title":"Adding Files to the Repository","text":"<p>Before you can track changes, you need to add your project's files to the repository. You can add individual files or all files in the directory using the following commands:</p> <p>Start tracking a file in our repository</p> Add a single fileAdd all files <pre><code>git add &lt;filename&gt;\n</code></pre> <pre><code>git add .\n</code></pre>"},{"location":"fundamentals/intro_to_git/#making-commits","title":"Making Commits","text":"<p>A commit is a snapshot of the changes you've made. To commit your changes, use the following command:</p> <p>Commit changes</p> <pre><code>git commit -m \"Your commit message\"\n</code></pre> <p>Make sure to replace \"Your commit message\" with a brief description of what you've changed.</p>"},{"location":"fundamentals/intro_to_git/#working-with-branches","title":"Working with Branches","text":"<p>Branches allow you to work on different features or parts of your project independently. </p> <p></p> <p>Create a new branch</p> <pre><code>git branch &lt;new-branch-name&gt;\n</code></pre> <p>Switch to a branch</p> <pre><code>git checkout &lt;new-branch-name&gt;\n</code></pre> <p>Create and Switch to a new branch in one command</p> <pre><code>git checkout -b &lt;new-branch-name&gt;\n</code></pre>"},{"location":"fundamentals/intro_to_git/#merging-branches","title":"Merging Branches","text":"<p>When you're done with a branch, you can merge it into the main branch (usually \"master\" or \"main\").</p> <p>How to merge a feature branch into the main branch</p> <ol> <li> <p>Switch to the main branch</p> <pre><code>git checkout main\n</code></pre> </li> <li> <p>Merge the other branch into the main branch</p> <pre><code>git merge new-branch-name\n</code></pre> </li> </ol>"},{"location":"fundamentals/intro_to_git/#sharing-your-code","title":"Sharing Your Code","text":"<p>You can share your GIT repository with others using platforms like GitHub or GitLab. Create an account on one of these platforms and follow their instructions for creating a new repository and pushing your code.</p> <p>Step 10: Fetching, Pulling and Pushing Changes</p> <p>How to fetch changes</p> <p>Fetching checks if there is any change available in the remote repository</p> <pre><code>git fetch\n</code></pre> <p>From a remote repository -&gt; Local repository</p> <pre><code>git pull\n</code></pre> <p>From local repository -&gt; remote repository</p> <pre><code>git push\n</code></pre> <p>That's it! You now have a basic understanding of GIT. Remember that GIT is a powerful tool with many features and commands, so this tutorial is just the beginning. As you become more comfortable with GIT, you can explore advanced topics such as resolving conflicts, branching strategies, and more.</p>"},{"location":"fundamentals/intro_to_poetry/","title":"Introduction to Poetry","text":"<p>Poetry is an essential tool for Python developers as it revolutionizes dependency management, packaging, and project development. It simplifies the process of managing project dependencies by providing a declarative and intuitive approach. With Poetry: </p> <ul> <li>Developers can easily define and track their project's dependencies, ensuring consistent and reproducible development environments across different machines. Additionally.</li> <li>Poetry simplifies packaging by automating the creation of distributable packages, making it easier to share and distribute projects with others.</li> <li>It also facilitates the creation of virtual environments, isolating project dependencies and avoiding conflicts. But, please better use specific virtual environment tools to create virtual-environments (pyenv, venv module, conda...)</li> </ul> <p>Poetry's comprehensive features, combined with its user-friendly interface, streamline the development workflow and enhance the overall efficiency and maintainability of Python projects.</p> <p>Why Poetry if we have plain <code>requirements.txt</code></p> <p>While <code>requirements.txt</code> is a simple declaration of needed dependencies with Poetry you can pretty much trust that whatever combination it finds will work. It also allows you to install the local project as a python module using pip (but this is a more advanced topic).</p> <p>More About Poetry</p> <p>You can find more info about Poetry in the official documentation.</p>"},{"location":"fundamentals/intro_to_poetry/#1-installing-poetry","title":"1. Installing Poetry","text":"<p>Once we have our development environment up and running, we can start creating our project. We will use Poetry to manage our project's dependencies and packaging. Poetry provides a simple and intuitive interface for managing dependencies, packaging, and virtual environments. It also offers a comprehensive set of features, such as dependency resolution, dependency isolation, and dependency locking. Poetry's user-friendly interface and powerful features make it an indispensable tool for Python developers.</p> <p>How to install poetry?</p> <p>Open a terminal and use onw of the following.</p> Using pipUsing the official installer <pre><code>pip install poetry==&lt;version&gt; # (1)\n</code></pre> <ol> <li>\u2139\ufe0f Remember to use a specific version (like <code>1.4.2</code>, <code>1.5.0</code> or <code>1.6.1</code>) by replacing <code>&lt;version&gt;</code> with the desired version.</li> </ol> <pre><code>curl -sSL https://install.python-poetry.org | python3 - --version &lt;version&gt; # (1)\n</code></pre> <ol> <li>\u2139\ufe0f Remember to use a specific version (like <code>1.4.2</code>, <code>1.5.0</code> or <code>1.6.1</code>) by replacing <code>&lt;version&gt;</code> with the desired version.</li> </ol> <p>Avoid creating a virtualenv using Poetry</p> <p>As we have seen before, by default Poetry creates a virtual environment for each project, but this is normally handled by other tools like <code>pyenv</code> or <code>conda</code>. So, if you are using Poetry. In order to avoid Poetry creating a virtual environment, you can use the <code>virtualenvs.create</code> config:</p> <pre><code>poetry config virtualenvs.create false\n</code></pre>"},{"location":"fundamentals/intro_to_poetry/#2-creating-a-new-project","title":"2. Creating a new project","text":"<p>To create a new project, we can use the <code>new</code> command:</p> <pre><code>poetry new &lt;project_name&gt;\n</code></pre> <p>The default poetry project structure</p> <p>The <code>poetry new &lt;project_name&gt;</code> command will create a new project with the following structure:</p> <pre><code>&lt;project_name&gt;\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 &lt;project_name&gt;\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 __init__.py\n</code></pre> <p>The default <code>pyproject.toml</code> file</p> <p>The <code>pyproject.toml</code> file contains the project's metadata and dependencies and it will look like this:</p> <pre><code>[tool.poetry]\nname = \"&lt;project_name&gt;\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"&lt;author_name&gt; &lt;author_email&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"&lt;project_name&gt;\"}]\n\n[tool.poetry.dependencies]\npython = \"^&lt;python_version&gt;\" # (1)\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n</code></pre> <ol> <li>The <code>^</code> symbol in poetry indicates version is pinned to the major version. Remember: versions are given as <code>MAJOR</code>.<code>MINOR</code>.<code>PATCH</code>. So, if we had, for example <code>^3.11</code> as the python version, it means that this project is compatible with any 3.x python version (being <code>x</code> equal or greater than 11) but never 4.x versions.</li> </ol>"},{"location":"fundamentals/intro_to_poetry/#32-adding-dependencies","title":"3.2. Adding dependencies","text":"<p>To add a dependency, we can use the <code>add</code> command:</p> <pre><code>poetry add &lt;dependency&gt;\n</code></pre> <p>How to add a main dependency: <code>requests</code></p> <p>This will add the dependency to the <code>pyproject.toml</code> file and install it in the virtual environment. For example, to add the <code>requests</code> library, we can use the following command:</p> <pre><code>poetry add requests\n</code></pre> <p>This will add the following lines to the <code>pyproject.toml</code> file:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.9\"\nrequests = \"^2.25.1\"\n</code></pre> <p>Adding dependencies just for development</p> <p>If we want to add a dependency just for development, we can use the <code>--group dev</code> flag. This will add the dependency to the <code>pyproject.toml</code> file under the <code>[tool.poetry.group.dev.dependencies]</code> section.</p> <pre><code>poetry add &lt;dependency&gt; --group dev\n</code></pre> <p>\"How to add a development dependency: <code>pytest</code></p> <p>For example, to add the <code>pytest</code> library, we can use the following command:</p> <pre><code>poetry add pytest --group dev\n</code></pre> <p>This will add the following lines to the <code>pyproject.toml</code> file:</p> <pre><code>[tool.poetry.dependencies]\npython = \"^3.11\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^6.2.2\"\n</code></pre>"},{"location":"fundamentals/intro_to_poetry/#33-removing-dependencies","title":"3.3. Removing dependencies","text":"<p>To remove a dependency, we can use the <code>remove</code> command:</p> <pre><code>poetry remove &lt;dependency&gt;\n</code></pre>"},{"location":"fundamentals/intro_to_virtual_environments/","title":"Introduction to Virtual Envs","text":"<p>A virtual environment is an isolated environment in which you can work on Python projects without interfering with the system-wide Python installation or other projects. This is an essential practice for managing dependencies and ensuring project-specific package versions. This is especially useful if you are working on multiple projects at the same time, or if you want to test out new packages without affecting your system. Here's how to create a virtual environment in Python, Some of the most common tools to create virtualenvs are:</p> <ul> <li>virtualenv</li> <li>pyenv</li> <li>conda</li> <li>pipenv</li> </ul> <p>Why Virtual Environments?</p> <p>Creating a virtual environment is a crucial step when working on Python projects to manage dependencies and keep your development environment clean and isolated. It allows you to work on multiple projects with different dependencies without conflicts.</p>"},{"location":"fundamentals/intro_to_virtual_environments/#using-venv-module","title":"Using <code>venv</code> Module","text":"<p>Venv module is a Python built-in module</p> <p>Starting from Python 3.3, a built-in module called <code>venv</code> can be used to create virtual environments. This method is recommended for Python 3.3 and later.</p> <p>1. Open a Terminal or Command Prompt.</p> <p>2. Create a Virtual Environment:</p> <p>Navigate to the directory where you want to create the virtual environment using <code>cd</code>:</p> <pre><code>cd /path/to/your/project\n</code></pre> <p>Create the virtual environment using the <code>venv</code> module. Replace <code>&lt;venv_name&gt;</code> with your desired environment name:</p> <pre><code>python -m venv &lt;venv_name&gt;\n</code></pre> <p>3. Activate the Virtual Environment:</p> <p>Command changes depending on the OS</p> <p>\ud83d\udc47 Select your operating system</p> On WindowsOn macOS and Linux <pre><code>&lt;venv_name&gt;\\Scripts\\activate\n</code></pre> <pre><code>source &lt;venv_name&gt;/bin/activate\n</code></pre> <p>How to Deactivate the Virtual Environment</p> <p>Simply run:</p> <pre><code>deactivate\n</code></pre>"},{"location":"fundamentals/intro_to_virtual_environments/#using-pyenv","title":"Using Pyenv","text":"<p>Virtualenvs are a great way to isolate your Python project dependencies. They allow you to create an isolated environment for your project, which means that you can install packages without affecting the rest of your system. </p> <p>How to install pyenv?</p> <p>Open a terminal (<code>Ctrl+Shift+`</code> or <code>Terminal &gt; New Terminal</code>) and use the following command:</p> <pre><code>curl https://pyenv.run | bash\n</code></pre> <p>1. Install the desired Python version</p> <pre><code>pyenv install &lt;python-version&gt;\n</code></pre> <p>2. Create a Virtualenv using Pyenv</p> <pre><code>pyenv virtualenv &lt;python-version&gt; &lt;virtualenv-name&gt;\n</code></pre> <p>3. Activate the Pyenv Virtualenv</p> <pre><code>pyenv activate &lt;virtualenv-name&gt;\n</code></pre> <p>Activate the virtualenv automatically</p> <p>You can activate the pyenv virtualenv automatically every time you enter into the project folder by using this command:</p> <pre><code>pyenv local &lt;virtualenv-name&gt;\n</code></pre> <p>This will create a <code>.python-version</code> file in the project structure.</p> <p>How to deactivate the virtualenv</p> <p>You can deactivate the virtualenv using </p> <pre><code>pyenv deactivate\n</code></pre>"},{"location":"fundamentals/intro_to_virtual_environments/#using-conda","title":"Using Conda","text":"<p>Conda is a popular package and environment management system that allows you to create isolated environments for different Python projects. In this tutorial, we'll walk you through the steps to create virtual environments using Conda.</p> <p>How to install conda?</p> <p>Before you begin, make sure you have Conda installed on your system. If you don't have Conda installed, you can download and install Miniconda or Anaconda, which includes Conda, from the official Conda website.</p> <p>1. Open a Terminal or Command Prompt</p> <p>2. Create a New Conda Environment</p> <p>To create a new Conda environment, use the <code>conda create</code> command. You can specify the Python version and additional packages during the environment creation. Replace <code>&lt;env_name&gt;</code> with your desired environment name and <code>&lt;python_version&gt;</code> with the Python version you want.</p> <pre><code>conda create --name &lt;env_name&gt; python=&lt;python_version&gt;\n</code></pre> <p>Venv named 'myenv' with python 3.11</p> <p>To create an environment named \"myenv\" with Python 3.11, you would run:</p> <pre><code>conda create --name myenv python=3.11\n</code></pre> <p>3. Activate the Conda Environment</p> <p>Once the environment is created, activate it using the <code>conda activate</code> command. Replace <code>&lt;env_name&gt;</code> with the name of the environment you want to activate.</p> <pre><code>conda activate &lt;env_name&gt;\n</code></pre> <p>Activate the 'myenv' environment</p> <pre><code>conda activate myenv\n</code></pre> <p>You will notice that the prompt changes to indicate that your Conda environment is active.</p> <p>How to deactivate the Conda Environment</p> <p>To deactivate the Conda environment and return to the base environment, simply run:</p> <pre><code>conda deactivate\n</code></pre> <p>List Conda Environments</p> <p>To view a list of all the Conda environments on your system, use the following command:</p> <pre><code>conda env list\n</code></pre> <p>Remove a Conda Environment</p> <p>To remove a Conda environment, you can use the <code>conda env remove</code> command. Replace <code>&lt;env_name&gt;</code> with the name of the environment you want to remove.</p> <pre><code>conda env remove --name &lt;env_name&gt;\n</code></pre> <p>remove the \"myenv\" environment</p> <pre><code>conda env remove --name myenv\n</code></pre>"},{"location":"mlflow/","title":"What is MLFlow?","text":"<p>Remember that you are a scientist (of data), and as a good scientist, you should keep a record of all your experiments. Well, imagine MLflow as a book where you can write down all your progress.</p> <p></p> <p>For example, imagine you are trying to develop a model that predicts the price of a house based on a series of parameters like the year of construction or the square footage. As a data scientist, your job is to research and experiment with different approaches to build an accurate model. This is very much like a writer working on their book.</p> <p></p> <p>This is the frontend of MLFlow. It is a web application that allows you to keep track of your experiments. You can see the different experiments you have run, the parameters you have used, the metrics you have measured, and the artifacts you have generated.</p>"},{"location":"mlflow/#how-to-install-mlflow","title":"How to install MLFlow?","text":"<p>It's really simple! \ud83d\ude03 You can install MLFlow using <code>pip</code> like any other python library (remember to do so inside a virtual environment)</p> <p>Install MLFlow</p> <pre><code>pip install mlflow\n</code></pre>"},{"location":"mlflow/#how-to-launch-mlflow","title":"How to launch MLFlow?","text":"<p>Once you have installed MLFlow, you can launch it with the following command:</p> <p>Launch MLFlow</p> <p>This will launch a server in your local machine. You can access it by going to http://localhost:5000 in your web browser.</p> <pre><code>mlflow server\n</code></pre> <p>You will see the following output:</p> <pre><code>[2024-02-21 23:29:52 +0100] [725738] [INFO] Starting gunicorn 21.2.0\n[2024-02-21 23:29:52 +0100] [725738] [INFO] Listening at: http://127.0.0.1:5000 (725738)\n[2024-02-21 23:29:52 +0100] [725738] [INFO] Using worker: sync\n[2024-02-21 23:29:52 +0100] [725739] [INFO] Booting worker with pid: 725739\n[2024-02-21 23:29:52 +0100] [725740] [INFO] Booting worker with pid: 725740\n[2024-02-21 23:29:53 +0100] [725741] [INFO] Booting worker with pid: 725741\n</code></pre>"},{"location":"mlflow/basics/1-connect_to_mlflow/","title":"Connect to MLFlow Server","text":"In\u00a0[10]: Copied! <pre>import mlflow\n\n\n# Default url for MLFlow is \"http://localhost:5000\"\nMLFLOW_TRACKING_URI = \"http://localhost:5000\" \n\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n</pre> import mlflow   # Default url for MLFlow is \"http://localhost:5000\" MLFLOW_TRACKING_URI = \"http://localhost:5000\"   mlflow.set_tracking_uri(MLFLOW_TRACKING_URI) <p>We can check the connection to the server by running the following command:</p> In\u00a0[11]: Copied! <pre># if not working this will raise an exception\nexperiments = mlflow.search_experiments()\n</pre> # if not working this will raise an exception experiments = mlflow.search_experiments()"},{"location":"mlflow/basics/1-connect_to_mlflow/#connect-to-mlflow-server","title":"Connect to MLFlow Server\u00b6","text":"<p>First we need to understand that mlflow runs in a server (like if it were another computer). Then we need to know the \"adress\" of that computer, think of it as if it where the adress of your house. This adress is known as the <code>MLFLOW TRACKING URI</code>. To set it up we have two ways:</p> <p></p>"},{"location":"mlflow/basics/1-connect_to_mlflow/#option-1-set-the-tracking-uri-at-the-beggining-of-our-code","title":"Option 1: Set the tracking URI at the beggining of our code\u00b6","text":""},{"location":"mlflow/basics/1-connect_to_mlflow/#option-2-set-the-tracking-uri-as-an-environment-variable","title":"Option 2: Set the tracking URI as an environment variable\u00b6","text":"<pre>export MLFLOW_TRACKING_URI=http://localhost:5000\n</pre>"},{"location":"mlflow/basics/2-creating_an_experiment/","title":"Create an Experiment","text":"<p>Creating an experiment in MLflow is simple. You just need to provide a name for the experiment, and MLflow will take care of the rest. If you want to create a new experiment, you can use the <code>mlflow.set_experiment</code> function. This function will create a new experiment with the specified name and return its ID. If an experiment with the same name already exists, it will raise an exception.</p> In\u00a0[4]: Copied! <pre>import mlflow\n\n\nexperiment = mlflow.set_experiment(\"mlflow-demo\")\n</pre> import mlflow   experiment = mlflow.set_experiment(\"mlflow-demo\") <p>We can see the details of the experiment by printing some of its attributes.</p> In\u00a0[5]: Copied! <pre>print(f\"Name: {experiment.name}\")\nprint(f\"Experiment_id: {experiment.experiment_id}\")\nprint(f\"Artifact Location: {experiment.artifact_location}\")\nprint(f\"Tags: {experiment.tags}\")\nprint(f\"Lifecycle_stage: {experiment.lifecycle_stage}\")\n</pre> print(f\"Name: {experiment.name}\") print(f\"Experiment_id: {experiment.experiment_id}\") print(f\"Artifact Location: {experiment.artifact_location}\") print(f\"Tags: {experiment.tags}\") print(f\"Lifecycle_stage: {experiment.lifecycle_stage}\") <pre>Name: mlflow-demo\nExperiment_id: 219393972832050318\nArtifact Location: file:///home/sngular/projects/codespace/mlops-cookbook/docs/basics/mlruns/219393972832050318\nTags: {}\nLifecycle_stage: active\n</pre>"},{"location":"mlflow/basics/2-creating_an_experiment/#create-an-experiment","title":"Create an Experiment\u00b6","text":""},{"location":"mlflow/basics/2-creating_an_experiment/#what-is-an-experiment","title":"What is an Experiment?\u00b6","text":"<p>An \"experiment\" in MLflow is like a specific chapter in your research book. It's an organized set of steps and records that detail how you've tested a particular approach in your machine learning project. Each experiment in MLflow includes the code you've used, the configured parameters, the input data, the results obtained, and any additional notes you want to add.</p> <p></p> <p>For example, if you are developing a model to predict house prices, an experiment in MLflow might involve training the model with a specific dataset, adjusting parameters like the number of trees in a random forest model, recording performance metrics like mean squared error (MSE), and making notes about any significant observations. Every time you try a different set of parameters or data, or even change the algorithm, you can create a new experiment in MLflow to maintain a clear record of your research and results. This makes it easier to track your progress and make informed decisions in your machine learning project.</p>"},{"location":"mlflow/basics/3-creating_a_run/","title":"Create a Run","text":"In\u00a0[1]: Copied! <pre>import mlflow\n\n\n# Set the experiment name\nexperiment = mlflow.set_experiment(\"mlflow-demo\")\n\n# Start a new run\nexperiment_id = experiment.experiment_id\nwith mlflow.start_run(experiment_id=experiment_id) as run:\n    print(f\"\ud83c\udf89 Run ID: {run.info.run_id}\")\n</pre> import mlflow   # Set the experiment name experiment = mlflow.set_experiment(\"mlflow-demo\")  # Start a new run experiment_id = experiment.experiment_id with mlflow.start_run(experiment_id=experiment_id) as run:     print(f\"\ud83c\udf89 Run ID: {run.info.run_id}\") <pre>\ud83c\udf89 Run ID: ee728d1da7e84c6e939b464419d9b85a\n</pre>"},{"location":"mlflow/basics/3-creating_a_run/#create-a-run","title":"Create a Run\u00b6","text":"<p>In the context of MLflow, a \"run\" refers to a specific instance of an experiment. You can think of a \"run\" as a unique execution of an experiment, where specific configurations are applied, a model is trained with particular data, and results are recorded.</p>"},{"location":"mlflow/basics/3-creating_a_run/#what-is-a-run","title":"What is a Run?\u00b6","text":"<p>Every time you perform a test, adjust parameters, or use a specific dataset within your MLflow experiment, you are creating a new \"run\" within that experiment. Each \"run\" keeps a detailed record of how parameters were configured, which data was used, and what results were obtained in that particular execution. In MLflow you can see all the runs within an experiment and compare them to determine which one is the most effective based on performance metrics and results.</p> <p></p> <p>The ability to track and compare multiple \"runs\" within the same experiment is valuable because it allows you to explore different approaches and settings, record the details of each attempt, and determine which one is the most effective based on performance metrics and results. In summary, a \"run\" in MLflow is an individual instance of an experiment that helps you keep an accurate track of your tests and progress in your machine learning project.</p>"},{"location":"mlflow/deployment/","title":"Model Deployment","text":"<p>Model deployment is the process of making your model available for use by other people or systems. When you serve a model, it's like putting it to work so that it can make predictions or classifications on new, incoming data.</p> <p>What does \"deploying a model\" mean?</p> <p>Model deployment is the process of making your model available for use by other people or systems. When you serve a model, it's like putting it to work so that it can make predictions or classifications on new, incoming data.</p>"},{"location":"mlflow/deployment/#model-deployment-workflow","title":"Model Deployment Workflow","text":"<p>Let's break it down using a simple analogy.</p> <p>Deploying a model (also called \"serving a model\") is a bit like setting up a restaurant (API) where you have a chef (your trained model) who can cook dishes (make predictions) for customers (users) based on their orders (incoming data).</p> <p>Comparing Serving a Model with a Chef in a Restaurant</p> <p>\ud83d\udc47 Click to compare</p> Regular Model ServingModel Deployment as if it were a restaurant <p></p> <p></p> <p>Here's how it works:</p> <ol> <li> <p>Training the Chef (Model): First, you train your chef by creating and fine-tuning your machine learning model using data. It's like teaching the chef how to cook your special recipes.</p> </li> <li> <p>Opening the Restaurant (Deployment): Once your chef is ready, you need a restaurant (a server or service) to serve the dishes (predictions). In MLflow, you can use the Model Registry to manage different versions of your chef's recipes (models) and choose which one to serve.</p> </li> <li> <p>Taking Orders (Incoming Data): People come to your restaurant and place orders (send data). For example, they might ask, \"Is this email spam?\" Your restaurant receives these orders (data) and sends them to your chef (model) to make predictions.</p> </li> <li> <p>Chef Makes Predictions (Model Serving): The chef (model) follows the recipe (model instructions) and prepares the dish (provides predictions) based on the order (incoming data).</p> </li> <li> <p>Serving the Dish (Sending Predictions): The chef serves the cooked dish (predictions) to the customers (applications or users) who requested it.</p> </li> </ol> <p>MLflow helps you manage and keep track of the chef's recipes (model versions) and ensures that everything runs smoothly in your \"restaurant\" by managing model deployments, making it easier to serve your machine learning models to applications and users in real time.</p>"},{"location":"mlflow/deployment/10-deploy_model_docker/","title":"Deploy a Model using Docker","text":"<p>Another way to deploy a model is using Docker. Docker is a platform for developing, shipping, and running applications using containerization. It allows you to package your application and its dependencies into a container that can run on any environment.</p> <p>Deploy a MLFlow model with Docker</p> <p>You can deploy a MLFlow model with Docker using the following command:</p> <pre><code>docker run \\\n    --net host \\\n    --env MLFLOW_TRACKING_URI=http://localhost:5000 \\\n    ghcr.io/mlflow/mlflow:v2.17.0 \\\n    mlflow models serve \\\n    --model-uri models:/&lt;model_name&gt;/&lt;model_version&gt; \\\n    --port 5001 \\\n    --env-manager local\n</code></pre> <p></p>"},{"location":"mlflow/deployment/10-deploy_model_locally/","title":"Deploy Model Locally","text":"<p>We've already explained that deploying a model can be seen as setting up a restaurant (API) where you have a chef (your trained model) who can cook dishes (make predictions) for customers (incoming data). Now the question is \"Where do you set up your restaurant (aka model)?\". The quick answer is that you will always deploy a model in a \"computer\" (server). It can be your computer (local deployment), a \"cloud computer\" (cloud server) or another \"computer\" inside your company (on-premise server).</p> <p>In a professional environment you will normally deploy your model on the cloud or on-premise servers. However, for testing purposes, you can deploy your model locally on your computer. This is what we will do in this tutorial.</p>"},{"location":"mlflow/deployment/10-deploy_model_locally/#deploy-a-model-using-mlflow","title":"Deploy a model using MLFLow","text":"<p>Mlflow provides a simple way to generate an API (the restaurant) using Flask with a simple command:</p> <p>Command to deploy a model from MLFlow</p> <pre><code>mlflow models serve --model-uri models:/&lt;model_name&gt;/&lt;model_stage&gt;\n</code></pre> <p>Flask must be installed</p> <p>Mlflow uses Flask (a well known python library) to generate the API, so you need to have Flask installed. You can install Flask using Pip or Poetry:</p> Using PipUsing Poetry <pre><code>pip install flask\n</code></pre> <pre><code>poetry add flask\n</code></pre>"},{"location":"mlflow/deployment/10-deploy_model_locally/#model-deployment-configuration","title":"Model Deployment Configuration","text":"<p>You can change the default configurations of the API by using the following options:</p> Option Description Default -m, --model-uri Required URI to the model. A local path, a 'runs:/' URI, or a remote storage URI (e.g., an 's3://' URI). For more information about supported remote URIs for model artifacts, see here. - -p, --port The port to listen on. 5000 -h, --host The network address to listen on. Use 0.0.0.0 to bind to all addresses if you want to access the tracking server from other machines. 127.0.0.1 -t, --timeout Timeout in seconds to serve a request. 60 -w, --workers Number of gunicorn worker processes to handle requests. 1 --env-manager If specified, create an environment for MLmodel using the specified environment manager. The following values are supported: - local: use the local environment - virtualenv: use virtualenv (and pyenv for Python version management) - conda: use conda If unspecified, default to virtualenv. virtualenv --no-conda If specified, use local environment. - --install-mlflow If specified and there is a conda or virtualenv environment to be activated, mlflow will be installed into the environment after it has been activated. The version of installed mlflow will be the same as the one used to invoke this command. - --enable-mlserver Enable serving with MLServer through the v2 inference protocol. You can use environment variables to configure MLServer. (See here) - <p>Let's see some examples \ud83d\ude0e:</p> <p>Changing the port of the API</p> <p>By default the port is set to the 5000, we can change it to the 8888 by:</p> <pre><code>mlflow models serve --model-uri models:/&lt;model_name&gt;/&lt;model_stage&gt; --port 8888\n</code></pre> <p>Change the timeout for serving requests</p> <p>By default the timeout is set to 60 seconds, we can change it to 10 seconds by:</p> <pre><code>mlflow models serve --model-uri models:/&lt;model_name&gt;/&lt;model_stage&gt; --timeout 10\n</code></pre>"},{"location":"mlflow/invocation/","title":"Model Request","text":"<p>Once the model is deployed into an API we need to learn to \"ask\" the model for predictions. This is called a model request.</p> <p>What is a request?</p> <p>A request is a message sent from a client to a server asking for a specific action to be performed. In the context of machine learning, a request is a message sent to a model asking for a prediction to be made. Following the analogy of a restaurant, a request is like a customer placing an order.</p> <p>Comparing a Model Request with a Customer Order</p> <p>\ud83d\udc47 Click to compare</p> Regular RequestRequest as if it were a restaurant <p></p> <p></p>"},{"location":"mlflow/invocation/#parts-of-a-request","title":"Parts of a Request","text":"<p>A request is composed of three main parts:</p> <ol> <li>The Request URL: This is the address where the model is deployed. It is like the address of the restaurant where you want to place your order.</li> <li>The Request Body: This is the data you send to the model to make a prediction. It is like the order you place at the restaurant.</li> <li>The Request Method: This is the type of request you are making. It can be <code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, etc. In the context of machine learning, you will mostly use <code>POST</code> and <code>GET</code> requests. For now think of it as the way you place your order at the restaurant.</li> </ol>"},{"location":"mlflow/invocation/calling_a_model_api/","title":"Call a Model API","text":"In\u00a0[1]: Copied! <pre>import requests\n\nENDPOINT = \"http://localhost:5001/ping\"\n\nresponse = requests.get(ENDPOINT)\nresponse\n</pre> import requests  ENDPOINT = \"http://localhost:5001/ping\"  response = requests.get(ENDPOINT) response Out[1]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[2]: Copied! <pre>import requests\n\nENDPOINT = \"http://127.0.0.1:5001/version\"\n\nresponse = requests.get(ENDPOINT)\nresponse.text\n</pre> import requests  ENDPOINT = \"http://127.0.0.1:5001/version\"  response = requests.get(ENDPOINT) response.text Out[2]: <pre>'2.17.0'</pre> In\u00a0[3]: Copied! <pre># Define the URL and payload (JSON data)\nENDPOINT = 'http://localhost:5001/invocations'\n</pre> # Define the URL and payload (JSON data) ENDPOINT = 'http://localhost:5001/invocations' In\u00a0[4]: Copied! <pre># we build de body (payload) of the request\nheaders = {'Content-Type': 'application/json'}\nfeatures = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]]  # list of lists (vectors)\nbody = {'inputs': features}\n</pre> # we build de body (payload) of the request headers = {'Content-Type': 'application/json'} features = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]]  # list of lists (vectors) body = {'inputs': features} In\u00a0[5]: Copied! <pre>import json\n\n# Convert the payload to JSON format\nbody_json = json.dumps(body)\n</pre> import json  # Convert the payload to JSON format body_json = json.dumps(body) In\u00a0[6]: Copied! <pre># Make a POST request\nresponse = requests.post(ENDPOINT, headers=headers, data=body_json)\nresponse.json()\n</pre> # Make a POST request response = requests.post(ENDPOINT, headers=headers, data=body_json) response.json() Out[6]: <pre>{'predictions': [-37.34314240221075]}</pre> <p>We can check that the status code of the response is 200, which means the request was successful.</p> In\u00a0[7]: Copied! <pre># Check the response\nresponse.status_code\n</pre> # Check the response response.status_code Out[7]: <pre>200</pre>"},{"location":"mlflow/invocation/calling_a_model_api/#call-a-model-api","title":"Call a Model API\u00b6","text":"<p>Once your model is deployed, you can call it using the <code>requests</code> library. MLFlow builds an API endpoint for your model, the endpoints are the following:</p> <ul> <li><code>POST /invocations</code>: An inference endpoint that accepts POST requests with input data and returns predictions.</li> <li><code>GET /ping</code>: Used for health checks.</li> <li><code>GET /health</code>: Same as /ping</li> <li><code>GET /version</code>: Returns the MLflow version.</li> </ul>"},{"location":"mlflow/invocation/calling_a_model_api/#check-the-model-status","title":"Check the model status\u00b6","text":"<p>First, we need to check if the model is running. We can do this by calling the <code>/ping</code> endpoint.</p>"},{"location":"mlflow/invocation/calling_a_model_api/#check-the-model-version","title":"Check the model version\u00b6","text":"<p>We can also check the model version by calling the <code>/version</code> endpoint.</p>"},{"location":"mlflow/invocation/calling_a_model_api/#call-the-model","title":"Call the model\u00b6","text":"<p>The model is ready to receive requests. We can call the <code>/invocations</code> endpoint with the input data to get the predictions. Let's go step by step:</p>"},{"location":"mlflow/invocation/calling_a_model_api/#1-define-the-endpoint-url","title":"1. Define the endpoint URL.\u00b6","text":""},{"location":"mlflow/invocation/calling_a_model_api/#2-prepare-the-data-to-be-sent","title":"2. Prepare the data to be sent\u00b6","text":"<p>Now we prepare the data to be sent to the model. When sending data remember we have 2 parts:</p> <ul> <li>Body: also called payload, it is the data we want to send to the model. The data should be in JSON format (in python, it is a dictionary). The JSON has a single key <code>inputs</code> and the value is a list of vectors. Each vector has 8 elements, those are the features of the model. The model will return a list of predictions, one for each input vector. The body is</li> <li>Headers: we need to specify the content type of the data we are sending. In this case, it is <code>application/json</code>.</li> </ul>"},{"location":"mlflow/invocation/calling_a_model_api/#3-convert-the-data-to-json","title":"3. Convert the data to JSON\u00b6","text":"<p>We need to convert the data to JSON format. We can use the <code>json</code> library to do this.</p>"},{"location":"mlflow/invocation/calling_a_model_api/#4-send-the-request","title":"4. Send the request\u00b6","text":"<p>Now we send the request (POST) to the model using the <code>requests</code> library. The response will be a JSON with the predictions.</p>"},{"location":"mlflow/registry/","title":"MLFlow Registry","text":"<p>The Model Registry is a hub where you can find the models that you have trained and saved as \"final\" models. It is a place where you can keep track of the different versions of your models, and it allows you to compare them, share them, and deploy them.</p> <p></p> <p>Models can be registered in two ways:</p> <ol> <li>Using the UI: You can register a model using the MLflow Web Intergace. This is useful when you want to manually register a model.</li> <li>Using the API: You can register a model using the MLflow API (using python code). This is useful when you want to automate the process of registering a model.</li> </ol>"},{"location":"mlflow/registry/8-model_registry/","title":"Model Registry","text":"In\u00a0[14]: Copied! <pre>import mlflow\n\n\nEXPERIMENT_NAME = \"mlflow-demo\"  # \u2757 make sure this experiment exists\n\n\n# get experiment id\nexperiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\nexperiment_id = experiment.experiment_id\n\n# get all runs taged with model=linear-regression in this experiment\nruns = mlflow.search_runs(\n    experiment_ids=experiment_id,\n    filter_string=\"tags.model = 'linear-regression'\",\n)\n\n# get the first retrieved run\nrun_id = runs.iloc[0].run_id\nprint(f\"\u2705 Using run_id '{run_id}'!\")\n</pre> import mlflow   EXPERIMENT_NAME = \"mlflow-demo\"  # \u2757 make sure this experiment exists   # get experiment id experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME) experiment_id = experiment.experiment_id  # get all runs taged with model=linear-regression in this experiment runs = mlflow.search_runs(     experiment_ids=experiment_id,     filter_string=\"tags.model = 'linear-regression'\", )  # get the first retrieved run run_id = runs.iloc[0].run_id print(f\"\u2705 Using run_id '{run_id}'!\") <pre>\u2705 Using run_id 'f1d8e8acaf354ef98561e5d8707161d0'!\n</pre> In\u00a0[15]: Copied! <pre># register the model for this run\nMODEL_NAME = \"demo-linear-regression\"  # change this to your model name\n\n\n# Compute model path: models stored in a run follow this convention\nmodel_path = f\"runs:/{run_id}/model\"  \n\n\n# register the model\nresult = mlflow.register_model(model_path, MODEL_NAME)\nprint(f\"\u2705 Registered model version: {result.version}!\")\n</pre> # register the model for this run MODEL_NAME = \"demo-linear-regression\"  # change this to your model name   # Compute model path: models stored in a run follow this convention model_path = f\"runs:/{run_id}/model\"     # register the model result = mlflow.register_model(model_path, MODEL_NAME) print(f\"\u2705 Registered model version: {result.version}!\") <pre>Registered model 'demo-linear-regression' already exists. Creating a new version of this model...\n2023/10/17 22:23:02 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: demo-linear-regression, version 4\n</pre> <pre>\u2705 Registered model version: 4!\n</pre> <pre>Created version '4' of model 'demo-linear-regression'.\n</pre>"},{"location":"mlflow/registry/8-model_registry/#model-registry","title":"Model Registry\u00b6","text":"<p>In this notebook, we will see how to register a model in MLflow. Registering a model is a way to keep track of the different versions of a model and its metadata. It is also a way to share the model with other people. When a model is registered a new version is created. The first version of a model is always version 1.</p>"},{"location":"mlflow/registry/8-model_registry/#search-a-model-in-a-run","title":"\ud83d\udd0d Search a Model in a Run\u00b6","text":"<p>We are going to register one of the model. We will use the <code>search_runs</code> method to find the run we want to register.</p>"},{"location":"mlflow/registry/8-model_registry/#register-the-model","title":"\ud83d\udea9 Register the Model\u00b6","text":""},{"location":"mlflow/registry/8-register_model_api/","title":"Register a Model using the API","text":"In\u00a0[\u00a0]: Copied! <pre>import mlflow\n\n\nMODEL_NAME = \"\"  # \ud83d\udc48 insert the name that you want for your model\nRUN_ID = \"\"  # \ud83d\udc48 insert the run id where the model is logged\n\n\n# Models stored in a run follow this convention\nmodel_path = f\"runs:/{RUN_ID}/model\"  \n\n\n# register the model\nresult = mlflow.register_model(model_path, MODEL_NAME)\nprint(f\"\u2705 Registered model version: {result.version}!\")\n</pre> import mlflow   MODEL_NAME = \"\"  # \ud83d\udc48 insert the name that you want for your model RUN_ID = \"\"  # \ud83d\udc48 insert the run id where the model is logged   # Models stored in a run follow this convention model_path = f\"runs:/{RUN_ID}/model\"     # register the model result = mlflow.register_model(model_path, MODEL_NAME) print(f\"\u2705 Registered model version: {result.version}!\")"},{"location":"mlflow/registry/8-register_model_api/#register-a-model-using-the-api","title":"Register a Model using the API\u00b6","text":"<p>Registering a model using the API is the same as doing it manually but using python code. This is useful when you want to automate the process of registering models.</p>"},{"location":"mlflow/registry/8-register_model_ui/","title":"Register a Model using the UI","text":"<p>The simplest way to register a model is by using the MLflow Web Interface. This is useful when you want to manually register a model. The instrucions shown here are were taken from the official documentation.</p>"},{"location":"mlflow/registry/8-register_model_ui/#step-1-open-the-model-details","title":"Step 1: Open the model details","text":"<p>By clicking on the model name in the desired run in the Experiments page, you can see the model details.</p> <p></p>"},{"location":"mlflow/registry/8-register_model_ui/#setp-2-register-the-model","title":"Setp 2: Register the model","text":"<p>You can click on the Register Model blue button to register the model.</p> <p></p> <p>Create a new model</p> <p>If you want to create a new lineage of a model, you can click on the \"Create New Version\" button. When a new model is registered a new version is created. The first version of a model is always version 1.</p>"},{"location":"mlflow/registry/9-tag_model_api/","title":"Tag a Model using the API","text":"In\u00a0[2]: Copied! <pre>import mlflow\n\n\nMODEL_NAME = \"\"  # insert the name of your model (make sure the model is registered)\nMODEL_VERSION = 1  # change to the model version you want to tag: 1, 2, 3, etc.\nMODEL_STAGE = \"\"  # insert the Alias\n\n\n# stage model\nclient = mlflow.MlflowClient()\ninfo = client.transition_model_version_stage(\n    name=MODEL_NAME,\n    version=MODEL_VERSION,\n    stage=MODEL_STAGE\n)\n\n# check current stage\nprint(info.current_stage)\n</pre> import mlflow   MODEL_NAME = \"\"  # insert the name of your model (make sure the model is registered) MODEL_VERSION = 1  # change to the model version you want to tag: 1, 2, 3, etc. MODEL_STAGE = \"\"  # insert the Alias   # stage model client = mlflow.MlflowClient() info = client.transition_model_version_stage(     name=MODEL_NAME,     version=MODEL_VERSION,     stage=MODEL_STAGE )  # check current stage print(info.current_stage) <pre>Production\n</pre>"},{"location":"mlflow/registry/9-tag_model_api/#tag-a-model-using-the-api","title":"Tag a Model using the API\u00b6","text":"<p>Give an alias to a model using the API is the same as doing it manually but using python code. This is usefull if you want to automate the process of tagging models.</p>"},{"location":"mlflow/registry/9-tag_model_ui/","title":"Tag a Model using the UI","text":"<p>When you register a model it automatically tagged with a version number (1, 2, 3,...). However you may want to add special tags, called \"Aliases\" to your model like \"testing\", \"production\"... This way you can easily know which model of your lineage is the one used in which environment.</p>"},{"location":"mlflow/registry/9-tag_model_ui/#step-1-open-the-model-details","title":"Step 1: Open the model details","text":"<p>Inside the \"Models\" tab click on the model name you want to tag.</p> <p></p>"},{"location":"mlflow/registry/9-tag_model_ui/#step-2-add-an-alias-to-the-model","title":"Step 2: Add an Alias to the model","text":"<p>Click on the pencil icon (\u270f\ufe0f) to edit the model aliases.</p> <p></p> <p>Why is tagging a model useful?</p> <p>By tagging your models you always have a same model name for a given environment despite its version (1, 2, 3...). This way you can have new versions of your model and always know which one is the one used in production, testing, etc.</p>"},{"location":"mlflow/tracking/4-logging_metrics/","title":"Log Metrics","text":"In\u00a0[1]: Copied! <pre>import mlflow\n\n\nEXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists\nRUN_NAME = \"run-with-metrics\"\n\n\nexperiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n\n\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=RUN_NAME,\n) as run:\n    \n    # set the tags\n    mlflow.set_tags({\n        \"model\": \"linear-regression\",\n        \"author\": \"mlops-cookbook\",\n    })\n    \n    # Log a parameter (key-value pair)\n    # Log the model parameters\n    mlflow.log_param(\"random_seed\", 42)\n    mlflow.log_param(\"train_size\", 0.7)\n\n    # Model training code here ...\n\n    # Log a metric; metrics can be updated throughout the run\n    mlflow.log_metric(\"precision\", 0.92)\n    mlflow.log_metric(\"recall\", 0.96)\n\n    # Print the run ID\n    print(f\"Run ID: {run.info.run_id}\")\n</pre> import mlflow   EXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists RUN_NAME = \"run-with-metrics\"   experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id   with mlflow.start_run(     experiment_id=experiment_id,     run_name=RUN_NAME, ) as run:          # set the tags     mlflow.set_tags({         \"model\": \"linear-regression\",         \"author\": \"mlops-cookbook\",     })          # Log a parameter (key-value pair)     # Log the model parameters     mlflow.log_param(\"random_seed\", 42)     mlflow.log_param(\"train_size\", 0.7)      # Model training code here ...      # Log a metric; metrics can be updated throughout the run     mlflow.log_metric(\"precision\", 0.92)     mlflow.log_metric(\"recall\", 0.96)      # Print the run ID     print(f\"Run ID: {run.info.run_id}\") <pre>Run ID: f7773173fca64d02bc55974d3c7762bb\n</pre>"},{"location":"mlflow/tracking/4-logging_metrics/#log-metrics","title":"Log Metrics\u00b6","text":"<p>In MLflow, you can log various types of data related to your experiments and runs. These logged data are crucial for tracking and understanding your machine learning processes. The main types of data that can be logged in MLflow include:</p> <ol> <li><p>Tags and Notes: You can add tags and notes to your \"runs\" to provide additional information and context about what you're doing in that specific experiment. This makes it easier to search and understand your \"runs\" in the future.</p> </li> <li><p>Parameters: These are configurable values used in the execution of a \"run.\" For example, the learning rate, maximum depth of a decision tree, or any other value that affects the behavior of your model.</p> </li> <li><p>Metrics: Metrics are quantitative measures of your model's performance. You can log metrics such as accuracy, mean squared error (MSE), logarithmic loss, etc. These metrics help you evaluate and compare the performance of different \"runs\" and approaches.</p> </li> </ol> <p>Logging these types of data in MLflow helps you maintain a detailed record of your experiments and enables you to analyze, compare, and effectively share your results. This is essential for the development of machine learning models as it allows you to understand which approaches work best and how to improve your work over time.</p> <p></p>"},{"location":"mlflow/tracking/5-logging_artifacts/","title":"Log Artifacts","text":"<p>Artifacts in MLflow are files or data that are important for your machine learning projects. They can be things like trained models, datasets, images, configuration files, or any other files that are relevant to your project.</p> <p>Artifacts are used in MLflow to track and manage the different versions of these files throughout the machine learning lifecycle. MLflow provides a centralized repository called the artifact store where you can store and retrieve these artifacts. By storing artifacts in MLflow, you can keep track of the exact version of the files used during each step of your machine learning workflow.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# generate a random confusion matrix\nconfusion_matrix = np.array([\n    [ 9,  1,  0],\n    [ 1,  9,  0],\n    [ 0,  0, 10]\n])\n\n# plot the confusion matrix\nax = sns.heatmap(confusion_matrix, cmap=\"Blues\", annot=True)\nax.set_xlabel(\"Predicted labels\")\nax.set_ylabel(\"True labels\")\nax.set_title(\"Confusion Matrix\")\n\n# save the confusion matrix to a file\nplt.savefig(\"confusion_matrix.png\")  # \ud83d\udc48 saving your artifact (remember artifacts are files)\n</pre> import numpy as np import seaborn as sns import matplotlib.pyplot as plt  # generate a random confusion matrix confusion_matrix = np.array([     [ 9,  1,  0],     [ 1,  9,  0],     [ 0,  0, 10] ])  # plot the confusion matrix ax = sns.heatmap(confusion_matrix, cmap=\"Blues\", annot=True) ax.set_xlabel(\"Predicted labels\") ax.set_ylabel(\"True labels\") ax.set_title(\"Confusion Matrix\")  # save the confusion matrix to a file plt.savefig(\"confusion_matrix.png\")  # \ud83d\udc48 saving your artifact (remember artifacts are files) In\u00a0[2]: Copied! <pre>import mlflow\n\n\nEXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists\nRUN_NAME = \"run-with-model\"\n\n\nexperiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n\n\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=RUN_NAME,\n) as run:\n    \n    # log the confusion matrix as an artifact\n    mlflow.log_artifact(\"confusion_matrix.png\")  # \ud83d\udc48 logging your artifact\n\n    # Print the run ID\n    print(f\"Run ID: {run.info.run_id}\")\n</pre> import mlflow   EXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists RUN_NAME = \"run-with-model\"   experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id   with mlflow.start_run(     experiment_id=experiment_id,     run_name=RUN_NAME, ) as run:          # log the confusion matrix as an artifact     mlflow.log_artifact(\"confusion_matrix.png\")  # \ud83d\udc48 logging your artifact      # Print the run ID     print(f\"Run ID: {run.info.run_id}\") <pre>Run ID: 5bce4f5add21457b80655b3b77482b45\n</pre>"},{"location":"mlflow/tracking/5-logging_artifacts/#log-artifacts","title":"Log Artifacts\u00b6","text":""},{"location":"mlflow/tracking/5-logging_artifacts/#create-a-simple-artifact","title":"\u2728 Create a simple artifact\u00b6","text":"<p>An image (png) of a confussion Matrix.</p>"},{"location":"mlflow/tracking/5-logging_artifacts/#log-the-artifact","title":"\ud83d\udcbe Log the artifact\u00b6","text":""},{"location":"mlflow/tracking/6-logging_models/","title":"Log Models","text":"<p>Model logging in MLflow refers to the practice of saving and tracking machine learning models during the development and experimentation process. When we log a model in MLflow, we save the model as an artifact in a centralized repository, allowing us to easily access and manage different versions of the model.</p> <p></p> <p>Model logging is important in MLflow for several reasons.</p> <ul> <li><p>Reproducibility: Logging models ensures that we can reproduce our experiments later on. By storing the exact version of the model used during training, we can accurately reproduce the same results or compare different model iterations.</p> </li> <li><p>Collaboration: MLflow allows teams to collaborate effectively by sharing models. By logging models, team members can easily access and deploy specific versions of the model, making it simpler to work together on projects.</p> </li> <li><p>Tracking: Model logging helps in tracking the development and progress of the model. It allows us to keep a record of the model's performance, metrics, and associated metadata, making it easier to analyze and compare different iterations or approaches.</p> </li> </ul> <p>When logging a model we specify the library used to create the model <code>model.&lt;library&gt;.log_model()</code>. Specifying the library used to create the model when logging helps ensure compatibility and consistency. Different machine learning libraries may have their own formats and conventions for storing models. By specifying the library used, MLflow can appropriately handle the model serialization and deserialization process, ensuring that the logged model can be loaded correctly when it is later accessed or deployed.</p> <p>In summary, model logging in MLflow involves saving and tracking machine learning models, providing benefits such as reproducibility, collaboration, and progress tracking. Specifying the library used to create the model ensures compatibility and consistency when storing and retrieving the models.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Mocked data\nX = np.random.rand(100, 1)  # Independent variable\ny = 2 * X + np.random.randn(100, 1)  # Dependent variable with some noise\n\n# Create and fit the linear regression model\nmodel = LinearRegression()\n_ = model.fit(X, y)\n</pre> import numpy as np from sklearn.linear_model import LinearRegression  # Mocked data X = np.random.rand(100, 1)  # Independent variable y = 2 * X + np.random.randn(100, 1)  # Dependent variable with some noise  # Create and fit the linear regression model model = LinearRegression() _ = model.fit(X, y) In\u00a0[3]: Copied! <pre>import mlflow\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")  # \u2757 set your tracking server URI\n\nEXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists\nRUN_NAME = \"run-with-model\"\n\n\nexperiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n\n\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=RUN_NAME,\n) as run:\n    \n    # log the model\n    mlflow.sklearn.log_model(model, \"linear_regression_model\")  # \ud83d\udc48 we tell mlflow is a sklearn model\n    mlflow.set_tags({\"model\": \"linear-regression\"})\n\n    # Print the run ID\n    print(f\"Run ID: {run.info.run_id}\")\n</pre> import mlflow  mlflow.set_tracking_uri(\"http://localhost:5000\")  # \u2757 set your tracking server URI  EXPERIMENT_NAME = \"mlflow-demo\"  #  \u2757 make sure this experiment exists RUN_NAME = \"run-with-model\"   experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id   with mlflow.start_run(     experiment_id=experiment_id,     run_name=RUN_NAME, ) as run:          # log the model     mlflow.sklearn.log_model(model, \"linear_regression_model\")  # \ud83d\udc48 we tell mlflow is a sklearn model     mlflow.set_tags({\"model\": \"linear-regression\"})      # Print the run ID     print(f\"Run ID: {run.info.run_id}\") <pre>Run ID: 585b6ec37839467d832d54b95886d6e9\n</pre> In\u00a0[6]: Copied! <pre>from mlflow.models.signature import infer_signature\n\n\n# Infer the signature of the model\nsignature = infer_signature(model_input=X, model_output=y)\n\n\n# Start a run to log the model with the signature\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=RUN_NAME,\n) as run:\n    \n    # log the model with the signature\n    mlflow.sklearn.log_model(model,\"linear_regression_model\", signature=signature)\n</pre> from mlflow.models.signature import infer_signature   # Infer the signature of the model signature = infer_signature(model_input=X, model_output=y)   # Start a run to log the model with the signature with mlflow.start_run(     experiment_id=experiment_id,     run_name=RUN_NAME, ) as run:          # log the model with the signature     mlflow.sklearn.log_model(model,\"linear_regression_model\", signature=signature)"},{"location":"mlflow/tracking/6-logging_models/#log-models","title":"Log Models\u00b6","text":""},{"location":"mlflow/tracking/6-logging_models/#create-a-simple-model","title":"\u2728 Create a simple Model\u00b6","text":"<p>A simple linear regression model trained on random data.</p>"},{"location":"mlflow/tracking/6-logging_models/#log-the-model","title":"\ud83d\udcbe Log the model\u00b6","text":""},{"location":"mlflow/tracking/6-logging_models/#model-signature","title":"\u270d\ufe0f Model Signature\u00b6","text":"<p>The model signature is a description of the input and output data types and shapes of the model. It is used to ensure that the model is used correctly when it is later loaded and deployed. The model signature is specified when logging the model, and it is stored as part of the model metadata.</p>"},{"location":"mlflow/tracking/7-autologging/","title":"Autologging","text":"In\u00a0[1]: Copied! <pre>import mlflow\n\n\nmlflow.autolog()\n</pre> import mlflow   mlflow.autolog()"},{"location":"mlflow/tracking/7-autologging/#autologging","title":"Autologging\u00b6","text":"<p>Autologging is a really cool MLFlow feature that allows you to automatically log metrics, parameters, and artifacts from your machine learning experiments. It simplifies the process of tracking your experiments and makes it easier to compare different runs.</p> <p>To use autologging, you just need to call <code>mlflow.autolog()</code> at the beginning of your script. This will automatically log all the metrics, parameters, and artifacts that are generated during the execution of your script.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Under the tutorials section you will find complete hands-on documentation:</p> <p>Tutorials are Jupyter Notebooks</p> <p>Tutorials are actually jupyter notebooks. Therefore you can download any tutorial and try it as a regular Jupyter Notebook. To Download a model simply click on the \u2b07\ufe0f button to the righthand side of the notebook's title.</p>"},{"location":"tutorials/#list-of-tutorials","title":"List of tutorials","text":"<ul> <li>Model Training using MLFlow</li> <li>Model Evaluation using MLFlow</li> </ul>"},{"location":"tutorials/1-full_model_training/","title":"Full Model Training","text":"<p>Welcome to the notebook that will guide you in using MLflow to register and track the training of a logistic regression model with the Titanic dataset. In this tutorial, we will explore how MLflow, an open-source platform for the machine learning lifecycle, can facilitate experiment tracking, metric monitoring, and model management in a reproducible environment.</p> <p>The Titanic dataset is a classic in the data science community, containing detailed information about passengers, such as their age, gender, ticket class, and whether they survived the disaster. We will use this information to build a logistic regression model capable of predicting the survival probability of a passenger based on their features.</p> <p>Throughout this notebook, we will not only focus on training the logistic regression model but also learn how to leverage the capabilities of MLflow. We will start by exploring the Titanic dataset, performing analysis and visualizations to better understand the distribution of variables and patterns present.</p> <p>Next, we will move on to the data preprocessing stage, where we will clean and transform the data to make it suitable for modeling. During this process, we will use MLflow to log the preprocessing steps, allowing us to have a complete and organized record of the transformations applied to the data.</p> <p>Then, we will dive into training the logistic regression using the training dataset. Here, MLflow will play a crucial role in logging training details, including the hyperparameters used, performance metrics, and other relevant aspects of the model. This will enable us to have a comprehensive overview of the process and facilitate comparison between different experiments and configurations.</p> <p>Once we have trained our model, we will use MLflow to register the model on the platform. This will allow us to have a complete record of the trained models, including the details of each model, such as the hyperparameters used, performance metrics, and associated source code. Additionally, MLflow will enable us to export the model in a standard format, making it easier to deploy in different environments.</p> <p>In summary, this notebook will provide you with the opportunity to work with MLflow and discover how this powerful tool can simplify and enhance the training process of a logistic regression model. Throughout the tutorial, you will learn to log experiments, metrics, and model details, allowing you to have a comprehensive and reproducible tracking of the entire process. So get ready to delve into Titanic data analysis while exploring the capabilities of MLflow in logistic regression training!</p> In\u00a0[1]: Copied! <pre>from datetime import datetime\n\nfrom sklearn.preprocessing import LabelEncoder  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nimport pandas as pd\nimport mlflow\n\nfrom mlops_course.settings import config\n</pre> from datetime import datetime  from sklearn.preprocessing import LabelEncoder   from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression import seaborn as sns import pandas as pd import mlflow  from mlops_course.settings import config In\u00a0[2]: Copied! <pre>DATASET_URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n\n\n# Load dataset\ndataframe = pd.read_csv(DATASET_URL)\ndataframe.head()\n</pre> DATASET_URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"   # Load dataset dataframe = pd.read_csv(DATASET_URL) dataframe.head() Out[2]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S In\u00a0[3]: Copied! <pre>dataframe.isnull().sum()\n</pre> dataframe.isnull().sum() Out[3]: <pre>PassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64</pre> <p>We see that there are missing values in the 'Age', 'Cabin', and 'Embarked' columns. We'll fill the missing values in the 'Age' column with the median age.</p> In\u00a0[4]: Copied! <pre># Fill age missing values\nmedian_age = dataframe['Age'].median()\ndataframe['Age'].fillna(median_age, inplace=True)\n</pre> # Fill age missing values median_age = dataframe['Age'].median() dataframe['Age'].fillna(median_age, inplace=True) <p>Sex is a categorical variable composed by \"male\" and \"female\" string values. In order to train our model we need to convert them to numerical values: male=1, female=0</p> In\u00a0[5]: Copied! <pre># convert string labels to numbers: male=1, female=0\nle = LabelEncoder()\ndataframe[\"Sex\"] = le.fit_transform(dataframe[\"Sex\"])\n</pre> # convert string labels to numbers: male=1, female=0 le = LabelEncoder() dataframe[\"Sex\"] = le.fit_transform(dataframe[\"Sex\"]) In\u00a0[6]: Copied! <pre>sns.pairplot(dataframe[[\"Age\", \"Sex\", \"Fare\", \"Survived\"]], hue=\"Survived\")\n</pre> sns.pairplot(dataframe[[\"Age\", \"Sex\", \"Fare\", \"Survived\"]], hue=\"Survived\") Out[6]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x7f9b740b76d0&gt;</pre> In\u00a0[7]: Copied! <pre>RANDOM_SEED = 42  # Set a random seed for reproducibility\nTEST_SIZE = 0.2  # Use 20% of the data for testing\n\n# Prepare the dataset for training\nfeatures = ['Sex', 'Age', 'Fare']\nX = dataframe[features]\nY = dataframe['Survived']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n</pre> RANDOM_SEED = 42  # Set a random seed for reproducibility TEST_SIZE = 0.2  # Use 20% of the data for testing  # Prepare the dataset for training features = ['Sex', 'Age', 'Fare'] X = dataframe[features] Y = dataframe['Survived']  # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_SEED) In\u00a0[8]: Copied! <pre># Set the URI where the MLflow server is running\nmlflow.set_tracking_uri(uri=config.MLFLOW_TRACKING_URI)\nprint(\"\u2705 Successfully connected to the MLflow server\")\n</pre> # Set the URI where the MLflow server is running mlflow.set_tracking_uri(uri=config.MLFLOW_TRACKING_URI) print(\"\u2705 Successfully connected to the MLflow server\") <pre>\u2705 Successfully connected to the MLflow server\n</pre> In\u00a0[9]: Copied! <pre>EXPERIMENT_NAME = \"Titanic Linear Regression\"  # change this to your experiment name\n\n\n# Create an experiment if it doesn't exist\ntry:\n    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n    print(f\"\u2705 Created '{EXPERIMENT_NAME}'!\")\nexcept mlflow.exceptions.RestException:\n    experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n    print(f\"\u2705 Experiment '{EXPERIMENT_NAME}' already exists!\")\n\nexperiment = mlflow.get_experiment(experiment_id)\n</pre> EXPERIMENT_NAME = \"Titanic Linear Regression\"  # change this to your experiment name   # Create an experiment if it doesn't exist try:     experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)     print(f\"\u2705 Created '{EXPERIMENT_NAME}'!\") except mlflow.exceptions.RestException:     experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id     print(f\"\u2705 Experiment '{EXPERIMENT_NAME}' already exists!\")  experiment = mlflow.get_experiment(experiment_id) <pre>\u2705 Experiment 'Titanic Linear Regression' already exists!\n</pre> In\u00a0[10]: Copied! <pre>run_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=run_name,\n) as run:\n    \n    # Train the logistic regression model\n    model = LogisticRegression(random_state=RANDOM_SEED)\n    model.fit(X_train.values, y_train)\n\n    # Log the model itself to MLflow\n    mlflow.sklearn.log_model(model, \"model\")\n\n    # Print the run ID\n    print(f\"Run ID: {run.info.run_id}\")\n</pre> run_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  with mlflow.start_run(     experiment_id=experiment_id,     run_name=run_name, ) as run:          # Train the logistic regression model     model = LogisticRegression(random_state=RANDOM_SEED)     model.fit(X_train.values, y_train)      # Log the model itself to MLflow     mlflow.sklearn.log_model(model, \"model\")      # Print the run ID     print(f\"Run ID: {run.info.run_id}\") <pre>Run ID: f14b6da63a47497eb837518537dfcdb2\n</pre> <pre>/usr/local/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n</pre>"},{"location":"tutorials/1-full_model_training/#full-model-training","title":"Full Model Training\u00b6","text":""},{"location":"tutorials/1-full_model_training/#get-the-data","title":"Get the Data\u00b6","text":"<p>We will load the dataset from a CSV file and display the first few rows to get an understanding of the data.</p>"},{"location":"tutorials/1-full_model_training/#read-the-data","title":"Read the data\u00b6","text":""},{"location":"tutorials/1-full_model_training/#clean-the-data","title":"Clean the Data\u00b6","text":""},{"location":"tutorials/1-full_model_training/#visualize-the-data","title":"Visualize the data\u00b6","text":""},{"location":"tutorials/1-full_model_training/#split-the-data","title":"Split the data\u00b6","text":""},{"location":"tutorials/1-full_model_training/#train-the-model","title":"Train the Model\u00b6","text":"<p>We'll train the model and we will use MLflow to log that model: logging a model means saving the model to a file or a database so that it can be used later for inference.</p>"},{"location":"tutorials/1-full_model_training/#connect-to-the-mlflow-server","title":"Connect to the MLflow server\u00b6","text":""},{"location":"tutorials/1-full_model_training/#create-the-experiment","title":"Create the experiment\u00b6","text":""},{"location":"tutorials/1-full_model_training/#train-the-model-and-log-it-to-mlflow","title":"Train the model and log it to MLflow\u00b6","text":""},{"location":"tutorials/2-full_model_evaluation/","title":"Full Model Evaluation","text":"<p>When we evaluate a model, we want to measure its performance and understand how accurate it is in making predictions. MLflow allows us to log different parameters, such as accuracy, precision, recall, and others, which help us understand how well the model is doing. It's like keeping score to see if the model is getting better or not.</p> <p>Additionally, MLflow allows us to save artifacts, which are files or resources that provide additional insights into the model's performance. These artifacts can include things like plots, charts, or even the trained model itself. It's like having a folder where we can save useful things related to the model.</p> <p>By logging parameters and saving artifacts with MLflow, we can easily keep track of the model's evaluation results and have all the necessary files in one place. This makes it easier to analyze the model's performance, share the evaluation results with others, and compare different versions of the model. It's like having a report card and a folder of useful materials to understand how good the model is and improve it if needed.</p> In\u00a0[1]: Copied! <pre>import mlflow\nfrom mlops_course.settings import config\nfrom sklearn.preprocessing import LabelEncoder  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sns\n</pre> import mlflow from mlops_course.settings import config from sklearn.preprocessing import LabelEncoder   from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, recall_score from datetime import datetime import pandas as pd import seaborn as sns  In\u00a0[2]: Copied! <pre>RANDOM_SEED = 42  # Set a random seed for reproducibility\nTEST_SIZE = 0.2  # 20% of the data will be used for testing\nDATASET_URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n\n# Load dataset\ndataframe = pd.read_csv(DATASET_URL)\n\n# Fill missing values\nage_mean = dataframe['Age'].mean()\ndataframe['Age'].fillna(age_mean, inplace=True)\nembarked_mode = dataframe['Embarked'].mode()[0]\ndataframe['Embarked'].fillna(embarked_mode, inplace=True)\n\n# convert string labels to numbers: male=1, female=0, embarked_C=0, embarked_Q=1, embarked_S=2\nle = LabelEncoder()\ndataframe[\"Sex\"] = le.fit_transform(dataframe[\"Sex\"])\ndataframe[\"Embarked\"] = le.fit_transform(dataframe[\"Embarked\"])\n\n# Prepare the dataset for training\nfeatures = ['Sex', 'Age', 'Fare', 'Embarked']\nX = dataframe[features]\nY = dataframe['Survived']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n</pre> RANDOM_SEED = 42  # Set a random seed for reproducibility TEST_SIZE = 0.2  # 20% of the data will be used for testing DATASET_URL = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"  # Load dataset dataframe = pd.read_csv(DATASET_URL)  # Fill missing values age_mean = dataframe['Age'].mean() dataframe['Age'].fillna(age_mean, inplace=True) embarked_mode = dataframe['Embarked'].mode()[0] dataframe['Embarked'].fillna(embarked_mode, inplace=True)  # convert string labels to numbers: male=1, female=0, embarked_C=0, embarked_Q=1, embarked_S=2 le = LabelEncoder() dataframe[\"Sex\"] = le.fit_transform(dataframe[\"Sex\"]) dataframe[\"Embarked\"] = le.fit_transform(dataframe[\"Embarked\"])  # Prepare the dataset for training features = ['Sex', 'Age', 'Fare', 'Embarked'] X = dataframe[features] Y = dataframe['Survived']  # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_SEED) In\u00a0[3]: Copied! <pre>sns.pairplot(dataframe[[\"Age\", \"Sex\", \"Fare\", \"Survived\"]], hue=\"Survived\")\n</pre> sns.pairplot(dataframe[[\"Age\", \"Sex\", \"Fare\", \"Survived\"]], hue=\"Survived\") Out[3]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x7efbef0191c0&gt;</pre> In\u00a0[4]: Copied! <pre># Train the logistic regression model\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\n</pre> # Train the logistic regression model model = LogisticRegression(random_state=42) model.fit(X_train, y_train) Out[4]: <pre>LogisticRegression(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression(random_state=42)</pre> In\u00a0[5]: Copied! <pre>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\n# generate the confusion matrix plot\ny_pred = model.predict(X_test)  # predict the test data\nconfusion_matrix_pred = confusion_matrix(y_pred, y_test)\nconfusion_matrix_plot = ConfusionMatrixDisplay(confusion_matrix_pred, display_labels=['Died', 'Survived']).plot()\n</pre> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay   # generate the confusion matrix plot y_pred = model.predict(X_test)  # predict the test data confusion_matrix_pred = confusion_matrix(y_pred, y_test) confusion_matrix_plot = ConfusionMatrixDisplay(confusion_matrix_pred, display_labels=['Died', 'Survived']).plot() In\u00a0[6]: Copied! <pre># Set the URI where the MLflow server is running\nmlflow.set_tracking_uri(uri=config.MLFLOW_TRACKING_URI)\n</pre> # Set the URI where the MLflow server is running mlflow.set_tracking_uri(uri=config.MLFLOW_TRACKING_URI) In\u00a0[7]: Copied! <pre>EXPERIMENT_NAME = \"Titanic Linear Regression\"  # change this to your experiment name\n\n\nexperiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\nrun_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nwith mlflow.start_run(\n    experiment_id=experiment_id,\n    run_name=run_name,\n) as run:\n\n    # Log the model parameters\n    mlflow.log_param(\"model_name\", \"Logistic Regression\")\n    mlflow.log_param(\"random_seed\", RANDOM_SEED)\n    mlflow.log_param(\"test_size\", TEST_SIZE)\n\n    # Train the logistic regression model\n    model = LogisticRegression(random_state=RANDOM_SEED)\n    model.fit(X_train, y_train)\n    mlflow.sklearn.log_model(model, \"model\")  # log the model itself\n\n    # Make predictions on the test set and compute metrics\n    y_pred = model.predict(X_test)\n    precision = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n\n    # Create a plot with the confusion matrix and log it as an artifact\n    confusion_matrix_pred = confusion_matrix(y_pred, y_test)\n    confusion_matrix_plot = ConfusionMatrixDisplay(confusion_matrix_pred, display_labels=['Died', 'Survived']).plot()\n    mlflow.log_figure(confusion_matrix_plot.figure_, \"confusion_matrix.png\")\n\n    # Print the run ID\n    print(f\"Run ID: {run.info.run_id}\")\n</pre> EXPERIMENT_NAME = \"Titanic Linear Regression\"  # change this to your experiment name   experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id run_name = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  with mlflow.start_run(     experiment_id=experiment_id,     run_name=run_name, ) as run:      # Log the model parameters     mlflow.log_param(\"model_name\", \"Logistic Regression\")     mlflow.log_param(\"random_seed\", RANDOM_SEED)     mlflow.log_param(\"test_size\", TEST_SIZE)      # Train the logistic regression model     model = LogisticRegression(random_state=RANDOM_SEED)     model.fit(X_train, y_train)     mlflow.sklearn.log_model(model, \"model\")  # log the model itself      # Make predictions on the test set and compute metrics     y_pred = model.predict(X_test)     precision = accuracy_score(y_test, y_pred)     recall = recall_score(y_test, y_pred)     mlflow.log_metric(\"precision\", precision)     mlflow.log_metric(\"recall\", recall)      # Create a plot with the confusion matrix and log it as an artifact     confusion_matrix_pred = confusion_matrix(y_pred, y_test)     confusion_matrix_plot = ConfusionMatrixDisplay(confusion_matrix_pred, display_labels=['Died', 'Survived']).plot()     mlflow.log_figure(confusion_matrix_plot.figure_, \"confusion_matrix.png\")      # Print the run ID     print(f\"Run ID: {run.info.run_id}\") <pre>/usr/local/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n</pre> <pre>Run ID: a902eaf4dfd64a29adc891ee574363b3\n</pre>"},{"location":"tutorials/2-full_model_evaluation/#full-model-evaluation","title":"Full Model Evaluation\u00b6","text":""},{"location":"tutorials/2-full_model_evaluation/#get-evaluation-data","title":"Get Evaluation Data\u00b6","text":""},{"location":"tutorials/2-full_model_evaluation/#visualize-the-data","title":"Visualize the Data\u00b6","text":"<p>Now let's visualize the relationship between various features and the target variable, 'Survived'.</p>"},{"location":"tutorials/2-full_model_evaluation/#train-the-model","title":"Train the model\u00b6","text":""},{"location":"tutorials/2-full_model_evaluation/#test-the-model","title":"Test the model\u00b6","text":""},{"location":"tutorials/2-full_model_evaluation/#connect-to-the-mlflow-server","title":"Connect to the MLflow server\u00b6","text":""},{"location":"tutorials/basic_model_tracking/","title":"Basic Model Tracking","text":"In\u00a0[17]: Copied! <pre>import numpy as np\n\n\nX = np.arange(100).reshape(100, 1)\ny = X ** 2\n</pre> import numpy as np   X = np.arange(100).reshape(100, 1) y = X ** 2 <p>Then select a simple linear regression model</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\n\nmodel = LinearRegression()\n</pre> from sklearn.linear_model import LinearRegression   model = LinearRegression() <p>Then we connect to the MLFlow server.</p> In\u00a0[5]: Copied! <pre>import mlflow\n\n\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n</pre> import mlflow   mlflow.set_tracking_uri(\"http://localhost:5000\") <p>Then we set the experiment.</p> In\u00a0[6]: Copied! <pre>experiment = mlflow.set_experiment(\"test\")\n</pre> experiment = mlflow.set_experiment(\"test\") <p>Then we'll use <code>autolog</code> to simplify the process of logging the model and the metrics into MLflow,</p> In\u00a0[7]: Copied! <pre>mlflow.sklearn.autolog()\n</pre> mlflow.sklearn.autolog() <p>Finally train the model and automatically log the metrics and the model.</p> In\u00a0[9]: Copied! <pre>experiment_id = experiment.experiment_id\nwith mlflow.start_run(experiment_id=experiment_id):\n    model.fit(X, y)\n</pre> experiment_id = experiment.experiment_id with mlflow.start_run(experiment_id=experiment_id):     model.fit(X, y) <pre>2024/10/12 17:59:03 INFO mlflow.tracking._tracking_service.client: \ud83c\udfc3 View run bittersweet-koi-963 at: http://localhost:5000/#/experiments/406653565916109575/runs/ef588dbfe5bd487abc3ca25eb5151e6f.\n2024/10/12 17:59:03 INFO mlflow.tracking._tracking_service.client: \ud83e\uddea View experiment at: http://localhost:5000/#/experiments/406653565916109575.\n</pre>"},{"location":"tutorials/basic_model_tracking/#basic-model-tracking","title":"Basic Model Tracking\u00b6","text":"<p>In this tutorial we'll show how to train a basic <code>sci-kit learn</code> model and log it into MLflow. First we need a vasic dataset:</p>"}]}